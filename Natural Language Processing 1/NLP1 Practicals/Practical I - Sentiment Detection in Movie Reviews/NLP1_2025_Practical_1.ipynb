{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002).\n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. were the \"inventors\" of the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7errgRASzZ"
   },
   "source": [
    "**Advice**\n",
    "\n",
    "Please read through the entire practical and familiarise\n",
    "yourself with all requirements before you start coding or otherwise\n",
    "solving the tasks. Writing clean and concise code can make the difference\n",
    "between solving the assignment in a matter of hours, and taking days to\n",
    "run all experiments.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused.\n",
    "\n",
    "Generally, any function that we already use or import in the notebook can be used. Besides those, importing something like `deepcopy` or `tqdm` that does not change the implementation of the algorithms is fine. Functions that change or simplify the way you would implement the algorithm, including text processing functions from libraries like `sklearn`, `pandas` or `nltk` are not allowed unless specified (e.g. `ngrams` and `PorterStemmer` from `ntlk`). If you have questions about any specific function or library, please discuss this with your TA who will be the one grading your assignment.\n",
    "\n",
    "## Environment\n",
    "\n",
    "All code should be written in **Python 3**.\n",
    "This is the default in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SaZnxptMJiD7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.19\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYZyIF7lJnGn"
   },
   "source": [
    "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
    "The easiest way to\n",
    "install Python is through downloading\n",
    "[Anaconda](https://www.anaconda.com/download).\n",
    "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
    "You can also use an IDE\n",
    "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
    "coding and debugging easier. It is good practice to create a [virtual\n",
    "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
    "project, so that any Python packages don’t interfere with other\n",
    "projects.\n",
    "\n",
    "\n",
    "**Learning Python 3**\n",
    "\n",
    "If you are new to Python 3, you may want to check out a few of these resources:\n",
    "- https://learnxinyminutes.com/docs/python3/\n",
    "- https://www.learnpython.org/\n",
    "- https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hok-BFu9lGoK"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import typing\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import tqdm\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWyGHwE-ieQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "**Download the sentiment lexicon and the movie reviews dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lm-rakqtlMOT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  647k  100  647k    0     0  1912k      0 --:--:-- --:--:-- --:--:-- 1925k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  6 79.6M    6 5201k    0     0  5075k      0  0:00:16  0:00:01  0:00:15 5089k\n",
      " 14 79.6M   14 11.3M    0     0  5724k      0  0:00:14  0:00:02  0:00:12 5731k\n",
      " 22 79.6M   22 17.5M    0     0  5940k      0  0:00:13  0:00:03  0:00:10 5946k\n",
      " 31 79.6M   31 24.9M    0     0  6335k      0  0:00:12  0:00:04  0:00:08 6339k\n",
      " 40 79.6M   40 32.2M    0     0  6555k      0  0:00:12  0:00:05  0:00:07 6593k\n",
      " 49 79.6M   49 39.1M    0     0  6654k      0  0:00:12  0:00:06  0:00:06 6978k\n",
      " 58 79.6M   58 46.4M    0     0  6764k      0  0:00:12  0:00:07  0:00:05 7186k\n",
      " 67 79.6M   67 53.4M    0     0  6816k      0  0:00:11  0:00:08  0:00:03 7346k\n",
      " 76 79.6M   76 60.6M    0     0  6882k      0  0:00:11  0:00:09  0:00:02 7324k\n",
      " 84 79.6M   84 67.5M    0     0  6903k      0  0:00:11  0:00:10  0:00:01 7252k\n",
      " 93 79.6M   93 74.4M    0     0  6920k      0  0:00:11  0:00:11 --:--:-- 7239k\n",
      "100 79.6M  100 79.6M    0     0  6946k      0  0:00:11  0:00:11 --:--:-- 7216k\n"
     ]
    }
   ],
   "source": [
    "# download sentiment lexicon\n",
    "!curl -L -o sent_lexicon https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
    "# download review data\n",
    "!curl -L -o reviews.json https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkPwuHp5LSuQ"
   },
   "source": [
    "**Load the movie reviews.**\n",
    "\n",
    "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "careEKj-mRpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000 \n",
      "\n",
      "0 NEG 29\n",
      "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
      "1 NEG 11\n",
      "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
      "2 NEG 24\n",
      "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
      "3 NEG 19\n",
      "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
      "4 NEG 38\n",
      "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
      "\n",
      "Number of word types: 47743\n",
      "Number of word tokens: 1512359\n",
      "\n",
      "Most common tokens:\n",
      "         , :    77842\n",
      "       the :    75948\n",
      "         . :    59027\n",
      "         a :    37583\n",
      "       and :    35235\n",
      "        of :    33864\n",
      "        to :    31601\n",
      "        is :    25972\n",
      "        in :    21563\n",
      "        's :    18043\n",
      "        it :    15904\n",
      "      that :    15820\n",
      "     -rrb- :    11768\n",
      "     -lrb- :    11670\n",
      "        as :    11312\n",
      "      with :    10739\n",
      "       for :     9816\n",
      "       his :     9542\n",
      "      this :     9497\n",
      "      film :     9404\n"
     ]
    }
   ],
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences,\n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "print(\"Total number of reviews:\", len(reviews), \"\\n\")\n",
    "\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "    print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "    print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "    print_sentence_with_pos(r[\"content\"][0])\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "    for sentence in review[\"content\"]:\n",
    "        for token, pos_tag in sentence:\n",
    "            c[token.lower()] += 1\n",
    "\n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "    print(\"%10s : %8d\" % (token, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PWaEoh8B34"
   },
   "source": [
    "# Lexicon-based approach (4pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTSMb6ma4E8"
   },
   "source": [
    "A traditional approach to automatically classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative) or a sentiment score (e.g., a score from 0 to 5).\n",
    "\n",
    "In this practical, you will use the sentiment\n",
    "lexicon released by Wilson et al. (2005). The path of the loaded lexicon is `\"sent_lexicon\"`.\n",
    "\n",
    "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
    "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
    "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
    "\n",
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. For those lemmas you are free to use any entry, so it is fine to only keep one, e.g. the first or last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ogq0Eq2hQglh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
     ]
    }
   ],
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    line_cnt = 0\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "        line_cnt += 1\n",
    "        if line_cnt > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mml4nOtIUBhn"
   },
   "source": [
    "Given such a sentiment lexicon, there are ways to solve\n",
    "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
    "$S_{binary}$ by counting how many words have a positive or a\n",
    "negative label in the sentiment lexicon $SLex$.\n",
    "\n",
    "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
    "\n",
    "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
    "\n",
    "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
    "\n",
    "$$\n",
    "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
    "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
    "        \\text{negative} & \\text{otherwise}\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFnMvbeeZrc"
   },
   "source": [
    "#### (Q1.1) Implement this classifier using the provided structure below. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHmmMK91n80C"
   },
   "outputs": [],
   "source": [
    "class SentimentLexicon:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lexicon_path: str = \"./sent_lexicon\",\n",
    "        threshold: float = 8.0,\n",
    "        weight: float = 1.0,\n",
    "    ) -> None:\n",
    "        \"\"\"Sentiment classifier using a lexicon approach.\n",
    "\n",
    "        Args:\n",
    "            lexicon_path (str, optional): the location of the sentiment lexicon file. Defaults to \"./sent_lexicon\".\n",
    "            threshold (float, optional): the threshold used in classification. Defaults to 8.0.\n",
    "            weight (float, optional): the weight assigned to words with strong polarity. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        self.lexicon_path = lexicon_path\n",
    "        self.threshold = threshold\n",
    "        self.weight = weight\n",
    "        self.lexicon: dict[str, dict[str, str]] = {}\n",
    "\n",
    "        # load and parse the sentiment lexicon file\n",
    "        with open(self.lexicon_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                # each line contains key=value pairs separated by whitespace\n",
    "                parts = line.strip().split()\n",
    "                entry = {}\n",
    "\n",
    "                # extract all key=value pairs\n",
    "                for part in parts:\n",
    "                    key, value = part.split(\"=\")\n",
    "                    entry[key] = value\n",
    "\n",
    "                # if a lemma (word1) appears multiple times with different pos tags (pos1), keep only its last entry\n",
    "                word = entry[\"word1\"].lower()\n",
    "                self.lexicon[word] = entry\n",
    "\n",
    "    def fit(\n",
    "        self, documents: list[list[str]], labels: list[typing.Literal[\"POS\", \"NEG\"]]\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the classifer according to the input features and target labels.\n",
    "\n",
    "        For the `SentimentLexicon` classifier, there are no parameters to learn.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "            labels (list[str]): the list of labels\n",
    "        \"\"\"\n",
    "        pass # nothing to fit for the lexicon-based classifier\n",
    "\n",
    "    def predict(self, documents: list[list[str]]) -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for doc in documents:\n",
    "            score = 0\n",
    "            # compute sentiment score for the document\n",
    "            for token in doc:\n",
    "                token = token.lower()\n",
    "                if token in self.lexicon:\n",
    "                    polarity = self.lexicon[token].get(\"priorpolarity\")\n",
    "                    subj_type = self.lexicon[token].get(\"type\")\n",
    "\n",
    "                    # apply stronger weight for words with strong polarity\n",
    "                    w = self.weight if subj_type == \"strongsubj\" else 1.0\n",
    "\n",
    "                    # update sentiment score\n",
    "                    if polarity == \"positive\":\n",
    "                        score += w\n",
    "                    elif polarity == \"negative\":\n",
    "                        score -= w\n",
    "\n",
    "                    # we ignore neutral words\n",
    "\n",
    "            # classify as positive if score exceeds threshold\n",
    "            label = \"POS\" if score >= self.threshold else \"NEG\"\n",
    "            predictions.append(label)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YryJZo_5n80C"
   },
   "source": [
    "#### (Q1.2) Implement functions to transform the dataset to the needed data structures. (0.5pt)\n",
    "\n",
    "In our current implementation, the classifier's `fit` and `predict` methods expect a list of corpus documents represented as `list[list[str]]`, and labels as a `list[str]`. However, our data is represented as `list[dict[str, Any]]`. Before we can fit and classify, we'll need to define a function to transform our data to the desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R96Ep9R-n80D"
   },
   "outputs": [],
   "source": [
    "def extract_labels(\n",
    "    documents: list[dict[str, typing.Any]],\n",
    ") -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "    \"\"\"Converts a list of reviews to a list of labels.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, Any]]): the reviews as a list of dicts.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: the labels as a list of str\n",
    "    \"\"\"\n",
    "    # extract the value of the 'sentiment' key from each review\n",
    "    labels = [doc[\"sentiment\"] for doc in documents]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def extract_unigrams(\n",
    "    documents: list[dict[str, typing.Any]], lower: bool = True\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"Converts a list of reviews to a list of unigram token lists.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, typing.Any]]): the reviews as a list of dicts.\n",
    "        lower (bool, optional): whether to lowercase the review tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: the list of unigram tokens\n",
    "    \"\"\"\n",
    "    # each review[\"content\"] is a list of sentences,\n",
    "    # where each sentence is a list of (token, pos) pairs\n",
    "    unigrams = []\n",
    "\n",
    "    for doc in documents:\n",
    "        tokens = []\n",
    "        for sentence in doc[\"content\"]:\n",
    "            for token, pos in sentence:\n",
    "                tokens.append(token.lower() if lower else token)\n",
    "        unigrams.append(tokens)\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2lyr9bkn80D"
   },
   "source": [
    "#### (Q1.3) Implement a function to compute the accuracy of a classification. (0.5pt)\n",
    "\n",
    "Finally, we need to implement a classification evaluation metric. For this practical, we'll be using [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_classification), which is defined as the proportion of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8_vxDC0sn80D"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred: list[str], y_true: list[str]) -> float:\n",
    "    \"\"\"Computes the accuracy score.\n",
    "\n",
    "    Args:\n",
    "        y_pred (list[str]): the predicted labels\n",
    "        y_true (list[str]): the ground truth labels\n",
    "\n",
    "    Returns:\n",
    "        float: the accuracy score\n",
    "    \"\"\"\n",
    "    # count how many predictions match the true labels\n",
    "    correct = sum(p == t for p, t in zip(y_pred, y_true))\n",
    "    # divide by total number of labels\n",
    "    return correct / len(y_true) if y_true else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBEgsK3nn80D"
   },
   "source": [
    "Now train, predict and evaluate your classifier using the implemented functions. Make sure to print the final accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XPyv_fvVn80D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review unigram tokens: ['two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '``', 'sorta', \"''\", 'find', 'out', '...', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '-lrb-', 'lost', 'highway', '&', 'memento', '-rrb-', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'did', \"n't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '``', 'normal', \"''\", 'but', 'then', 'downshifts', 'into', 'this', '``', 'fantasy', \"''\", 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'do', \"n't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'s\", 'biggest', 'problem', '.', 'it', \"'s\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '``', 'into', 'it', \"''\", 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!!', 'okay', ',', 'we', 'get', 'it', '...', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'ve\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '``', 'the', 'suits', \"''\", 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'s\", 'unraveling', '.', 'overall', ',', 'the', 'film', 'does', \"n't\", 'stick', 'because', 'it', 'does', \"n't\", 'entertain', ',', 'it', \"'s\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '...', 'it', \"'s\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '...', 'skip', 'it', '!']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "X_uni = extract_unigrams(reviews)\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review unigram tokens:\", X_uni[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the lexicon-based approach: 0.6790\n"
     ]
    }
   ],
   "source": [
    "# initialize the lexicon-based classifier with the given threshold and no weight\n",
    "lexicon_clf = SentimentLexicon(lexicon_path=\"sent_lexicon\", threshold=8.0, weight=1.0)\n",
    "\n",
    "# no need to fit for the lexicon-based classifier\n",
    "# lexicon_clf.fit(X_uni, y)\n",
    "\n",
    "# predict sentiments\n",
    "lexicon_clf_y_pred = lexicon_clf.predict(X_uni)\n",
    "\n",
    "# evaluate accuracy\n",
    "lexicon_clf_acc = accuracy(lexicon_clf_y_pred, y)\n",
    "\n",
    "print(f\"Test accuracy for the lexicon-based approach: {lexicon_clf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twox0s_3eS0V"
   },
   "source": [
    "#### (Q1.4) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1 pt)\n",
    "\n",
    "As the sentiment lexicon also has information about the **magnitude** of\n",
    "sentiment (e.g., *“excellent\"* would have higher magnitude than\n",
    "*“good\"*), we can take a more fine-grained approach by adding up all\n",
    "sentiment scores, and deciding the polarity of the movie review using\n",
    "the sign of the weighted score $S_{weighted}$.\n",
    "\n",
    "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9vVk7CvDpyka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the weighted lexicon-based approach: 0.6830\n"
     ]
    }
   ],
   "source": [
    "# initialize the lexicon-based classifier with magnitude weighting\n",
    "weighted_lexicon_clf = SentimentLexicon(lexicon_path=\"sent_lexicon\", threshold=8.0, weight=2.0)\n",
    "\n",
    "# no need to fit for the weighted lexicon-based classifier\n",
    "# weighted_lexicon_clf.fit(X_uni, y)\n",
    "\n",
    "# predict sentiments\n",
    "weighted_lexicon_clf_y_pred = weighted_lexicon_clf.predict(X_uni)\n",
    "\n",
    "# evaluate accuracy\n",
    "weighted_lexicon_clf_acc = accuracy(weighted_lexicon_clf_y_pred, y)\n",
    "\n",
    "print(f\"Test accuracy for the weighted lexicon-based approach: {weighted_lexicon_clf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's test different weight values to see which one gives the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.1, Accuracy: 0.5780\n",
      "Weight: 0.5, Accuracy: 0.6410\n",
      "Weight: 1, Accuracy: 0.6790\n",
      "Weight: 2.0, Accuracy: 0.6830\n",
      "Weight: 3.0, Accuracy: 0.6855\n",
      "Weight: 4.0, Accuracy: 0.6835\n",
      "Weight: 5.0, Accuracy: 0.6840\n",
      "Weight: 10.0, Accuracy: 0.6795\n",
      "Weight: 15.0, Accuracy: 0.6770\n",
      "Weight: 25.0, Accuracy: 0.6745\n",
      "\n",
      "Best weight: 3.0, Best Accuracy: 0.6855\n"
     ]
    }
   ],
   "source": [
    "# list of weights to try\n",
    "weights = [0.1, 0.5, 1, 2.0, 3.0, 4.0,5.0, 10.0, 15.0, 25.0]\n",
    "\n",
    "best_weight = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for weight in weights:\n",
    "    # initialize the lexicon-based classifier with the given weight\n",
    "    lexicon_clf = SentimentLexicon(lexicon_path=\"sent_lexicon\", threshold=8.0, weight=weight)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = lexicon_clf.predict(X_uni)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y)\n",
    "    print(f\"Weight: {weight}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # select best weight based on accuracy\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_weight = weight\n",
    "\n",
    "print(f\"\\nBest weight: {best_weight}, Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhS8OCVxMHd"
   },
   "source": [
    "#### (Q1.5) A better threshold (1pt)\n",
    "\n",
    "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
    "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_uSlPv2n80E"
   },
   "source": [
    "> The fixed threshold of 8 works fairly well, but it doesn’t take into account that longer reviews naturally contain more sentiment words. Because of that, they’re more likely to reach or exceed the threshold even if the overall tone is mixed, while shorter reviews might be marked as negative simply because they don’t have enough words to build up the score.\n",
    ">\n",
    "> To make this fairer, we normalize the sentiment score by the number of words that actually appear in the sentiment lexicon. This way, we’re looking at the **average sentiment per meaningful word** against a small bias that is used for dataset imbalance, and gives more balanced results for both short and long reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n9DpkyYn80E"
   },
   "source": [
    "Now implement your algorithm by adjusting the previously created `SentimentLexicon` class. Make sure to print the final accuracy score and compare to the previous results.\n",
    "\n",
    "Note: you might need to adjust your threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "xxnBsTxnn80E"
   },
   "outputs": [],
   "source": [
    "class BetterSentimentLexicon(SentimentLexicon):\n",
    "    def predict(self, documents: list[list[str]]) -> list[str]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        bias = 0.1  # small bias to offset dataset imbalance\n",
    "\n",
    "        for doc in documents:\n",
    "            score = 0.0\n",
    "            lexicon_tokens = 0  # count only words found in the lexicon\n",
    "\n",
    "            # compute sentiment score for the document\n",
    "            for token in doc:\n",
    "                token = token.lower()\n",
    "                if token in self.lexicon:\n",
    "                    polarity = self.lexicon[token].get(\"priorpolarity\")\n",
    "                    subj_type = self.lexicon[token].get(\"type\")\n",
    "\n",
    "                    # apply stronger weight for words with strong polarity\n",
    "                    w = self.weight if subj_type == \"strongsubj\" else 1.0\n",
    "\n",
    "                    if polarity == \"positive\":\n",
    "                        score += w\n",
    "                    elif polarity == \"negative\":\n",
    "                        score -= w\n",
    "\n",
    "                    lexicon_tokens += 1\n",
    "\n",
    "            # normalize only if there are lexicon words\n",
    "            if lexicon_tokens > 0:\n",
    "                norm_score = score / lexicon_tokens\n",
    "            else:\n",
    "                norm_score = 0.0\n",
    "\n",
    "            # classify with bias threshold\n",
    "            label = \"POS\" if norm_score > bias else \"NEG\"\n",
    "            predictions.append(label)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "JBvPGZKan80E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the better lexicon-based approach: 0.6920\n"
     ]
    }
   ],
   "source": [
    "# initialize the lexicon-based classifier no threshold and best weight\n",
    "better_lexicon_clf = BetterSentimentLexicon(lexicon_path=\"sent_lexicon\", weight=3.0)\n",
    "\n",
    "# no need to fit for the better lexicon-based classifier\n",
    "# better_lexicon_clf.fit(X_uni, y)\n",
    "\n",
    "# predict sentiments\n",
    "better_lexicon_clf_y_pred = better_lexicon_clf.predict(X_uni)\n",
    "\n",
    "# evaluate accuracy\n",
    "better_lexicon_clf_acc = accuracy(better_lexicon_clf_y_pred, y)\n",
    "\n",
    "print(f\"Test accuracy for the better lexicon-based approach: {better_lexicon_clf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We created a table that compares the performance of all lexicon-based sentiment classifiers tested so far. Each variation uses the same dataset and evaluation method, differing only in the use of weighting or threshold adjustments.\n",
    "> \n",
    "> | Approach | Weight | Threshold | Accuracy |\n",
    "> |:----------|:-------:|:-----------:|:----------:|\n",
    "> | Basic Lexicon (Q1.3) | 1.0 | 8.0 | 0.6790 |\n",
    "> | Weighted Lexicon (Q1.4) | 2.0 | 8.0 | 0.6830 |\n",
    "> | Best Weight Search | 3.0 | 8.0 | 0.6855 |\n",
    "> | Better Lexicon (Q1.5) | 3.0 | 0.1 (small bias) | **0.6920** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9adQnuwia"
   },
   "source": [
    "# Naive Bayes (10pt)\n",
    "\n",
    "Your second task is to program a simple Machine Learning approach that operates\n",
    "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
    "described by Pang et al. (2002). In this approach, the only features we\n",
    "will consider are the words in the text themselves, without bringing in\n",
    "external sources of information. The BoW model is a popular way of\n",
    "representing texts as vectors, making it\n",
    "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
    "However, the BoW representation is also very crude, since it discards\n",
    "all information related to word order and grammatical structure in the\n",
    "original text—as the name suggests.\n",
    "\n",
    "## Writing your own classifier (4pts)\n",
    "\n",
    "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
    "a reminder, the Naive Bayes classifier works according to the following\n",
    "equation:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
    "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
    "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
    "vector. Remember that we use the log of these probabilities when making\n",
    "a prediction:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
    "\n",
    "You can find more details about Naive Bayes in [Jurafsky &\n",
    "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
    "this helpful\n",
    "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "*Note: this section and the next aim to put you in a position to replicate\n",
    "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
    "    will differ from theirs, as they used different data.*\n",
    "\n",
    "**You must write the Naive Bayes training and prediction code from\n",
    "scratch.** You will not be given credit for using off-the-shelf Machine\n",
    "Learning libraries.\n",
    "\n",
    "The data contains the text of the reviews, where each document consists\n",
    "of the sentences in the review, the sentiment of the review and an index\n",
    "(cv) that you will later use for cross-validation. The\n",
    "text has already been tokenised and POS-tagged for you. Your algorithm\n",
    "should read in the text, **lowercase it**, store the words and their\n",
    "frequencies in an appropriate data structure that allows for easy\n",
    "computation of the probabilities used in the Naive Bayes algorithm, and\n",
    "then make predictions for new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpyQSBSkb33"
   },
   "source": [
    "#### (Q2.1) Unseen words (1pt)\n",
    "The presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
    "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanFiYYnoxDW"
   },
   "source": [
    "> If we skip unseen words only for one class (for instance, when a word appears only in positive reviews during training but not in negative ones), we would **bias the prediction toward that class**.\n",
    "> \n",
    "> This happens because the class that includes the word would still gain probability mass from it, while the other class would not, leading to an **unfair comparison of log-probabilities**.\n",
    "> \n",
    "> By skipping the word for **both classes**, we ensure that both sides are treated equally and the decision is based only on words that were observed in **both sentiment classes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZRhaI3WvzC"
   },
   "source": [
    "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. (2pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUMXCeRTn80F"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing: float = 0):\n",
    "        \"\"\"Naive Bayes classifier for multinomial data.\n",
    "\n",
    "        Args:\n",
    "            smoothing (float, optional): the Laplacian smoothing factor. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        # The datastructures necessary for NaiveBayes\n",
    "        self.vocab: set[str] = set()\n",
    "\n",
    "        self.prior: dict[typing.Literal[\"POS\", \"NEG\"], float] = {\n",
    "            \"POS\": 0,\n",
    "            \"NEG\": 0,\n",
    "        }\n",
    "\n",
    "        self.token_counts: dict[str, dict[typing.Literal[\"POS\", \"NEG\"], int]] = (\n",
    "            defaultdict(\n",
    "                lambda: {\"POS\": 0, \"NEG\": 0},\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.cond_prob: dict[str, dict[typing.Literal[\"POS\", \"NEG\"], float]] = (\n",
    "            defaultdict(\n",
    "                lambda: {\"POS\": 0.0, \"NEG\": 0.0},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        documents: list[list[typing.Any]],\n",
    "        labels: list[typing.Literal[\"POS\", \"NEG\"]],\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the classifer according to the input features and target labels.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens\n",
    "            labels (list[str]): the list of labels\n",
    "        \"\"\"\n",
    "        num_docs = len(documents)\n",
    "        class_counts = Counter(labels)\n",
    "        classes = [\"POS\", \"NEG\"]\n",
    "\n",
    "        # compute prior probability P(c) for each class c\n",
    "        for c in classes:\n",
    "            self.prior[c] = class_counts[c] / num_docs\n",
    "\n",
    "        # compute token counts per class\n",
    "        for doc, label in zip(documents, labels):\n",
    "            for token in doc:\n",
    "                token_lower = token.lower()\n",
    "                self.vocab.add(token_lower)\n",
    "                self.token_counts[token_lower][label] += 1\n",
    "\n",
    "        # compute total number of tokens per class\n",
    "        total_tokens = {\n",
    "            \"POS\": sum(self.token_counts[w][\"POS\"] for w in self.vocab),\n",
    "            \"NEG\": sum(self.token_counts[w][\"NEG\"] for w in self.vocab),\n",
    "        }\n",
    "\n",
    "        V = len(self.vocab)\n",
    "        alpha = self.smoothing\n",
    "\n",
    "        # compute conditional probabilities P(word|class) with optional Laplacian smoothing\n",
    "        for token in self.vocab:\n",
    "            for c in classes:\n",
    "                count_wc = self.token_counts[token][c]\n",
    "                total_count_c = total_tokens[c]\n",
    "                self.cond_prob[token][c] = (count_wc + alpha) / (total_count_c + alpha * V)\n",
    "\n",
    "    def predict(\n",
    "        self, documents: list[list[typing.Any]]\n",
    "    ) -> list[typing.Literal[\"POS\", \"NEG\"]]:\n",
    "        \"\"\"Perform classification on input documents.\n",
    "\n",
    "        Args:\n",
    "            documents (list[list[str]]): the list of documents as a list of tokens\n",
    "\n",
    "        Returns:\n",
    "            list[str]: predicted labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for doc in documents:\n",
    "            scores = {}\n",
    "\n",
    "            for c in [\"POS\", \"NEG\"]:\n",
    "\n",
    "                # compute log-probability for class c for the test document\n",
    "                if self.prior[c] == 0:\n",
    "                    log_prob_c = float(\"-inf\")\n",
    "                else:\n",
    "                    log_prob_c = math.log(self.prior[c])\n",
    "\n",
    "                for token in doc:\n",
    "                    token_lower = token.lower()\n",
    "                    # skip unseen test words for both classes as discussed in (Q2.1)\n",
    "                    pos_seen = self.token_counts[token_lower][\"POS\"] > 0\n",
    "                    neg_seen = self.token_counts[token_lower][\"NEG\"] > 0\n",
    "\n",
    "                    if not (pos_seen and neg_seen) and self.smoothing == 0.0:\n",
    "                        continue\n",
    "                    \n",
    "                    # compute conditional log probabilities logP(word|class) \n",
    "                    prob_wc = self.cond_prob[token_lower][c]\n",
    "                    if prob_wc > 0:\n",
    "                        log_prob_c += math.log(prob_wc)\n",
    "                    \n",
    "                scores[c] = log_prob_c\n",
    "\n",
    "            # assign the class with the higher log-probability\n",
    "            predicted_class = max(scores, key=scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAc3OKHxn80F"
   },
   "source": [
    "Now train, predict and evaluate your classifier on the reviews dataset. Make sure to select the right reviews for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset based on cv index\n",
    "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900] # cv < 900 for training \n",
    "test_reviews = [r for r in reviews if int(r[\"cv\"]) >= 900] # cv >= 900 for testing\n",
    "\n",
    "# extract train data\n",
    "X_train_nb = extract_unigrams(train_reviews)\n",
    "y_train_nb = extract_labels(train_reviews)\n",
    "\n",
    "# extract test data\n",
    "X_test_nb = extract_unigrams(test_reviews)\n",
    "y_test_nb = extract_labels(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "XKKRBL2Rn80F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the Naive Bayes Classifier with no Laplacian Smoothing: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# initialize the naive bayes classifier with no laplacian smoothing\n",
    "nb_no_smooth = NaiveBayes(smoothing=0.0)\n",
    "\n",
    "# fit the naive bayes classifier\n",
    "nb_no_smooth.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "# predict sentiments\n",
    "y_pred_nb_no_smooth = nb_no_smooth.predict(X_test_nb)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_nb_no_smooth = accuracy(y_pred_nb_no_smooth, y_test_nb)\n",
    "\n",
    "print(f\"Test accuracy for the Naive Bayes Classifier with no Laplacian Smoothing: {acc_nb_no_smooth:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0INK-PBoM6CB"
   },
   "source": [
    "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
    "\n",
    "Simulate this scenario by keeping the positive reviews\n",
    "data unchanged, but only using negative reviews cv000–cv089 for\n",
    "training, and cv900–cv909 for testing. Calculate the classification\n",
    "accuracy, and explain what changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFbcsYlipBAw"
   },
   "source": [
    "> When 90% of the data is positive, **accuracy becomes misleading** because a classifier can achieve high accuracy simply by predicting the majority class (\"POS\") every time. This happens even if it completely fails to identify negative reviews. \n",
    "> \n",
    "> In such imbalanced scenarios, metrics like **precision, recall, or F1-score** are more informative, as they better reflect performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9zmChKqnn80F"
   },
   "outputs": [],
   "source": [
    "train_reviews_imb = [\n",
    "    r for r in reviews\n",
    "    if (r[\"sentiment\"] == \"POS\" and int(r[\"cv\"]) < 900)  # keep all positive training reviews\n",
    "    or (r[\"sentiment\"] == \"NEG\" and int(r[\"cv\"]) <= 89)   # only negatives cv000–cv089\n",
    "]\n",
    "\n",
    "test_reviews_imb = [\n",
    "    r for r in reviews\n",
    "    if (r[\"sentiment\"] == \"POS\" and int(r[\"cv\"]) >= 900)  # keep all positive test reviews\n",
    "    or (r[\"sentiment\"] == \"NEG\" and 900 <= int(r[\"cv\"]) <= 909)  # only negatives cv900–cv909\n",
    "]\n",
    "\n",
    "# extract train data\n",
    "X_train_nb_imb = extract_unigrams(train_reviews_imb)\n",
    "y_train_nb_imb = extract_labels(train_reviews_imb)\n",
    "\n",
    "# extract test data\n",
    "X_test_nb_imb = extract_unigrams(test_reviews_imb)\n",
    "y_test_nb_imb = extract_labels(test_reviews_imb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the Naive Bayes Classifier with No Laplacian Smoothing for Imbalanced Data: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# initialize the naive bayes classifier with no laplacian smoothing for imbalanced data\n",
    "nb_imb = NaiveBayes(smoothing=0.0)\n",
    "\n",
    "# fit the naive bayes classifier\n",
    "nb_imb.fit(X_train_nb_imb, y_train_nb_imb)\n",
    "\n",
    "# predict sentiments\n",
    "y_pred_nb_imb = nb_imb.predict(X_test_nb_imb)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_nb_imb = accuracy(y_pred_nb_imb, y_test_nb_imb)\n",
    "\n",
    "print(f\"Test accuracy for the Naive Bayes Classifier with No Laplacian Smoothing for Imbalanced Data: {acc_nb_imb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When the dataset became imbalanced, the classifier’s accuracy dropped from about 0.82 to 0.60. This happened because the model was trained with far fewer negative examples and therefore learned to predict the positive class more often. As a result, accuracy decreased and became less meaningful as an evaluation metric, since it mainly reflects the dominant positive class rather than true performance across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wJzcHX3WUDm"
   },
   "source": [
    "## Smoothing (1pt)\n",
    "\n",
    "As mentioned above, the presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive\n",
    "Bayes classifier to be $0$, thus making that particular test instance\n",
    "undecidable. The standard way to mitigate this effect (as well as to\n",
    "give more clout to rare words) is to use smoothing, in which the\n",
    "probability fraction\n",
    "\n",
    "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$\n",
    "\n",
    "for a word $w_i$ becomes\n",
    "\n",
    "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBNIcbwUWphC"
   },
   "source": [
    "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
    "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
    "Bayes classifier’s code, and report the accuracy for $\\kappa = 1$ and $\\kappa = 3.5$. Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "g03yflCc9kpW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the Naive Bayes Classifier with κ=1 Laplacian Smoothing: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# initialize the naive bayes classifier with κ=1 laplacian smoothing\n",
    "nb_smooth_1 = NaiveBayes(smoothing=1.0)\n",
    "\n",
    "# fit the naive bayes classifier\n",
    "nb_smooth_1.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "# predict sentiments\n",
    "y_pred_nb_smooth_1 = nb_smooth_1.predict(X_test_nb)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_nb_smooth_1 = accuracy(y_pred_nb_smooth_1, y_test_nb)\n",
    "\n",
    "print(f\"Test accuracy for the Naive Bayes Classifier with κ=1 Laplacian Smoothing: {acc_nb_smooth_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for the Naive Bayes Classifier with κ=3.5 Laplacian Smoothing: 0.8450\n"
     ]
    }
   ],
   "source": [
    "# initialize the naive bayes classifier with κ=3.5 laplacian smoothing\n",
    "nb_smooth_3_5 = NaiveBayes(smoothing=3.5)\n",
    "\n",
    "# fit the naive bayes classifier\n",
    "nb_smooth_3_5.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "# predict sentiments\n",
    "y_pred_nb_smooth_3_5 = nb_smooth_3_5.predict(X_test_nb)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_nb_smooth_3_5 = accuracy(y_pred_nb_smooth_3_5, y_test_nb)\n",
    "\n",
    "print(f\"Test accuracy for the Naive Bayes Classifier with κ=3.5 Laplacian Smoothing: {acc_nb_smooth_3_5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXF3oGVwn80G"
   },
   "source": [
    "**FROM NOW ON, ALWAYS USE SMOOTHING (YOU CAN CHOOSE $\\kappa$) WHEN USING THE NAIVE BAYES CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGcgwba87D5"
   },
   "source": [
    "## Cross-Validation (1pt)\n",
    "\n",
    "A serious danger in using Machine Learning on small datasets, with many\n",
    "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
    "suggested by the data” errors. This type of error occurs when we make\n",
    "repeated improvements to our classifiers by playing with features and\n",
    "their processing, but we don’t get a fresh, never-before seen test\n",
    "dataset every time. Thus, we risk developing a classifier that gets better\n",
    "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
    "\n",
    "A simple method to guard against Type III errors is to use\n",
    "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
    "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
    "time holding out one of the folds for testing, training our classifier\n",
    "on the remaining N - 1 data folds, and reporting performance on the\n",
    "held-out fold. We can use different strategies for dividing the data:\n",
    "\n",
    "-   Consecutive splitting:\n",
    "  - cv000–cv099 = Split 1\n",
    "  - cv100–cv199 = Split 2\n",
    "  - etc.\n",
    "  \n",
    "-   Round-robin splitting (mod 10):\n",
    "  - cv000, cv010, cv020, … = Split 1\n",
    "  - cv001, cv011, cv021, … = Split 2\n",
    "  - etc.\n",
    "\n",
    "-   Random sampling/splitting\n",
    "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OeLcbSauGtR"
   },
   "source": [
    "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the mean and standard deviation of the accuracies. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "FKhv5-uRn80H"
   },
   "outputs": [],
   "source": [
    "def cross_validation_splits(\n",
    "    documents: list[dict[str, typing.Any]],\n",
    "    num_folds: int = 10,\n",
    ") -> list[tuple[list, list]]:\n",
    "    \"\"\"Splits the reviews into disjoint subsets of train/test splits.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, typing.Any]]): the reviews as dicts.\n",
    "        num_folds (int, optional): the number of cross-validation folds. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[list, list]]: a list of train/test folds\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "\n",
    "    # group reviews by their cv index modulo num_folds\n",
    "    for i in range(num_folds):\n",
    "        # test set: all reviews where cv number % num_folds == i\n",
    "        test = [doc for doc in documents if int(doc[\"cv\"]) % num_folds == i]\n",
    "        # train set: all other reviews\n",
    "        train = [doc for doc in documents if int(doc[\"cv\"]) % num_folds != i]\n",
    "\n",
    "        folds.append((train, test))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbzmBa0In80H"
   },
   "source": [
    "Now re-train the Naive Bayes classifier on each fold, and evaluate on that fold's held-out test set. Make sure to print the accuracy mean and standard deviation.\n",
    "\n",
    "Note: you may use `tqdm` to add progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "0-RyhL_3n80H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  10%|█         | 1/10 [00:11<01:45, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  20%|██        | 2/10 [00:18<01:11,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  30%|███       | 3/10 [00:20<00:40,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  40%|████      | 4/10 [00:23<00:26,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  50%|█████     | 5/10 [00:24<00:16,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  60%|██████    | 6/10 [00:25<00:10,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  70%|███████   | 7/10 [00:27<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  80%|████████  | 8/10 [00:29<00:04,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation:  90%|█████████ | 9/10 [00:31<00:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation: 100%|██████████| 10/10 [00:33<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.8150\n",
      "\n",
      "=== 10-Fold Cross-Validation Results (Naive Bayes, κ=3.5) ===\n",
      "Mean accuracy: 0.8240\n",
      "Standard deviation: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean, stdev\n",
    "\n",
    "folds = cross_validation_splits(reviews, num_folds=10)\n",
    "nb_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation\")):\n",
    "    # extract train data\n",
    "    X_train = extract_unigrams(train_docs)\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract test data\n",
    "    X_test = extract_unigrams(test_docs)\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "    # initialize naive bayes classifier with κ=3.5 laplacian smoothing\n",
    "    nb = NaiveBayes(smoothing=3.5)\n",
    "\n",
    "    # fit the naive bayes classifier\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    nb_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== 10-Fold Cross-Validation Results (Naive Bayes, κ=3.5) ===\")\n",
    "print(f\"Mean accuracy: {mean(nb_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(nb_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v-Dl_1Jn80H"
   },
   "source": [
    "**FROM NOW ON, ALWAYS USE CROSS VALIDATION WHEN EVALUATING A CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6A2zX9_BRKm"
   },
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKK8FNt8VtcZ"
   },
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and\n",
    "hash different inflections of a word to the same feature in the BoW\n",
    "vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpsc1fId-5IE"
   },
   "source": [
    "#### (Q2.6): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
    "\n",
    "First we'll need to adjust our `extract_unigrams` function to extract stemmed unigrams. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "tA_XoWeNn80I"
   },
   "outputs": [],
   "source": [
    "def extract_stems(\n",
    "    documents: list[dict[str, typing.Any]], stemmer: PorterStemmer, lower: bool = True\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"Extract stemmed unigrams from a corpus of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict[str, Any]]): the list of documents\n",
    "        stemmer (PorterStemmer): the stemmer to use when stemming documents\n",
    "        lower (bool, optional): whether to lowercase tokens before stemming. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: the list of token lists\n",
    "    \"\"\"\n",
    "    # each review[\"content\"] is a list of sentences,\n",
    "    # where each sentence is a list of (token, pos) pairs\n",
    "    stems = []\n",
    "\n",
    "    for doc in documents:\n",
    "        stemmed_tokens = []\n",
    "        for sentence in doc[\"content\"]:\n",
    "            for token, pos in sentence:\n",
    "                # lowercase if needed, then stem\n",
    "                word = token.lower() if lower else token\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                stemmed_tokens.append(stemmed_word)\n",
    "        stems.append(stemmed_tokens)\n",
    "\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review stem tokens: ['two', 'teen', 'coupl', 'go', 'to', 'a', 'church', 'parti', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accid', '.', 'one', 'of', 'the', 'guy', 'die', ',', 'but', 'hi', 'girlfriend', 'continu', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'ha', 'nightmar', '.', 'what', \"'s\", 'the', 'deal', '?', 'watch', 'the', 'movi', 'and', '``', 'sorta', \"''\", 'find', 'out', '...', 'critiqu', ':', 'a', 'mind-fuck', 'movi', 'for', 'the', 'teen', 'gener', 'that', 'touch', 'on', 'a', 'veri', 'cool', 'idea', ',', 'but', 'present', 'it', 'in', 'a', 'veri', 'bad', 'packag', '.', 'which', 'is', 'what', 'make', 'thi', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'sinc', 'i', 'gener', 'applaud', 'film', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '-lrb-', 'lost', 'highway', '&', 'memento', '-rrb-', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'way', 'of', 'make', 'all', 'type', 'of', 'film', ',', 'and', 'these', 'folk', 'just', 'did', \"n't\", 'snag', 'thi', 'one', 'correctli', '.', 'they', 'seem', 'to', 'have', 'taken', 'thi', 'pretti', 'neat', 'concept', ',', 'but', 'execut', 'it', 'terribl', '.', 'so', 'what', 'are', 'the', 'problem', 'with', 'the', 'movi', '?', 'well', ',', 'it', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simpli', 'too', 'jumbl', '.', 'it', 'start', 'off', '``', 'normal', \"''\", 'but', 'then', 'downshift', 'into', 'thi', '``', 'fantasi', \"''\", 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audienc', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'go', 'on', '.', 'there', 'are', 'dream', ',', 'there', 'are', 'charact', 'come', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'other', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strang', 'apparit', ',', 'there', 'are', 'disappear', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scene', ',', 'there', 'are', 'ton', 'of', 'weird', 'thing', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simpli', 'not', 'explain', '.', 'now', 'i', 'person', 'do', \"n't\", 'mind', 'tri', 'to', 'unravel', 'a', 'film', 'everi', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'doe', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'thi', 'film', \"'s\", 'biggest', 'problem', '.', 'it', \"'s\", 'obvious', 'got', 'thi', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seem', 'to', 'want', 'to', 'hide', 'it', 'complet', 'until', 'it', 'final', 'five', 'minut', '.', 'and', 'do', 'they', 'make', 'thing', 'entertain', ',', 'thrill', 'or', 'even', 'engag', ',', 'in', 'the', 'meantim', '?', 'not', 'realli', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flick', 'like', 'thi', ',', 'so', 'we', 'actual', 'figur', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strang', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'littl', 'bit', 'of', 'sens', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertain', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movi', 'like', 'thi', 'is', 'that', 'you', 'should', 'alway', 'make', 'sure', 'that', 'the', 'audienc', 'is', '``', 'into', 'it', \"''\", 'even', 'befor', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understand', '.', 'i', 'mean', ',', 'show', 'melissa', 'sagemil', 'run', 'away', 'from', 'vision', 'for', 'about', '20', 'minut', 'throughout', 'the', 'movi', 'is', 'just', 'plain', 'lazi', '!!', 'okay', ',', 'we', 'get', 'it', '...', 'there', 'are', 'peopl', 'chase', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'realli', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'give', 'us', 'differ', 'scene', 'offer', 'further', 'insight', 'into', 'all', 'of', 'the', 'strang', 'go', 'down', 'in', 'the', 'movi', '?', 'appar', ',', 'the', 'studio', 'took', 'thi', 'film', 'away', 'from', 'it', 'director', 'and', 'chop', 'it', 'up', 'themselv', ',', 'and', 'it', 'show', '.', 'there', 'might', \"'ve\", 'been', 'a', 'pretti', 'decent', 'teen', 'mind-fuck', 'movi', 'in', 'here', 'somewher', ',', 'but', 'i', 'guess', '``', 'the', 'suit', \"''\", 'decid', 'that', 'turn', 'it', 'into', 'a', 'music', 'video', 'with', 'littl', 'edg', ',', 'would', 'make', 'more', 'sens', '.', 'the', 'actor', 'are', 'pretti', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'we', 'bentley', 'just', 'seem', 'to', 'be', 'play', 'the', 'exact', 'same', 'charact', 'that', 'he', 'did', 'in', 'american', 'beauti', ',', 'onli', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudo', 'go', 'out', 'to', 'sagemil', ',', 'who', 'hold', 'her', 'own', 'throughout', 'the', 'entir', 'film', ',', 'and', 'actual', 'ha', 'you', 'feel', 'her', 'charact', \"'s\", 'unravel', '.', 'overal', ',', 'the', 'film', 'doe', \"n't\", 'stick', 'becaus', 'it', 'doe', \"n't\", 'entertain', ',', 'it', \"'s\", 'confus', ',', 'it', 'rare', 'excit', 'and', 'it', 'feel', 'pretti', 'redund', 'for', 'most', 'of', 'it', 'runtim', ',', 'despit', 'a', 'pretti', 'cool', 'end', 'and', 'explan', 'to', 'all', 'of', 'the', 'crazi', 'that', 'came', 'befor', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'thi', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '...', 'it', \"'s\", 'just', 'packag', 'to', 'look', 'that', 'way', 'becaus', 'someon', 'is', 'appar', 'assum', 'that', 'the', 'genr', 'is', 'still', 'hot', 'with', 'the', 'kid', '.', 'it', 'also', 'wrap', 'product', 'two', 'year', 'ago', 'and', 'ha', 'been', 'sit', 'on', 'the', 'shelv', 'ever', 'sinc', '.', 'whatev', '...', 'skip', 'it', '!']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# stemming example\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "X_stem = extract_stems(reviews, stemmer)\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review stem tokens:\", X_stem[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdYfIJsjn80I"
   },
   "source": [
    "Now re-train your Naive Bayes classifier. Use cross-validation to evaluate the classifier. Use nltk's `PorterStemmer` to stem the document tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "wq-X7wokn80I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  10%|█         | 1/10 [00:38<05:47, 38.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  20%|██        | 2/10 [01:03<04:04, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  30%|███       | 3/10 [01:30<03:23, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  40%|████      | 4/10 [01:57<02:47, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  50%|█████     | 5/10 [02:25<02:20, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  60%|██████    | 6/10 [03:00<02:01, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  70%|███████   | 7/10 [03:30<01:31, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  80%|████████  | 8/10 [04:00<01:00, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming:  90%|█████████ | 9/10 [04:32<00:30, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Stemming: 100%|██████████| 10/10 [05:29<00:00, 32.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.8250\n",
      "\n",
      "=== 10-Fold Cross-Validation Results (Naive Bayes, κ=3.5, Stemming) ===\n",
      "Mean accuracy: 0.8210\n",
      "Standard deviation: 0.0237\n"
     ]
    }
   ],
   "source": [
    "# initialize porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stem_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation with Stemming\")):\n",
    "    # extract stemmed train data\n",
    "    X_train = extract_stems(train_docs, stemmer)\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract stemmed test data\n",
    "    X_test = extract_stems(test_docs, stemmer)\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "    # initialize naive bayes classifier with κ=3.5 laplacian smoothing\n",
    "    nb = NaiveBayes(smoothing=3.5)\n",
    "\n",
    "    # fit the naive bayes classifier\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    stem_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== 10-Fold Cross-Validation Results (Naive Bayes, κ=3.5, Stemming) ===\")\n",
    "print(f\"Mean accuracy: {mean(stem_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(stem_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkDHVq_1XUVP"
   },
   "source": [
    "#### (Q2.7) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
    "\n",
    "Give actual numbers. You can use the training set from Q2.4 to determine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jcPh63Cn80I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size without Stemming: 45348\n",
      "Vocabulary Size with Stemming: 32404\n",
      "Reduction: 12944 fewer features (28.54% reduction)\n"
     ]
    }
   ],
   "source": [
    "# use the same training set from q2.4\n",
    "X_train_unigrams = extract_unigrams(train_reviews) # this is the same as X_train_nb\n",
    "X_train_stems = extract_stems(train_reviews, PorterStemmer())\n",
    "\n",
    "# compute vocab sizes\n",
    "vocab_unigrams = set(token for doc in X_train_unigrams for token in doc)\n",
    "vocab_stems = set(token for doc in X_train_stems for token in doc)\n",
    "\n",
    "print(f\"Vocabulary Size without Stemming: {len(vocab_unigrams)}\")\n",
    "print(f\"Vocabulary Size with Stemming: {len(vocab_stems)}\")\n",
    "print(f\"Reduction: {len(vocab_unigrams) - len(vocab_stems)} fewer features ({100 * (1 - len(vocab_stems)/len(vocab_unigrams)):.2f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq"
   },
   "source": [
    "### N-grams (2.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to add **n-gram** features.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu"
   },
   "source": [
    "#### (Q2.8) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (2pt)\n",
    "\n",
    "We'll need to adjust our `extract_unigrams` function to extract n-grams. Use the provided structure below. You can use nltk's `ngrams` function to extract n-grams from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "6ZyHrHL7n80K"
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(\n",
    "    documents: list[dict], n_values: typing.Sequence[int], lower: bool = True\n",
    ") -> list[list[typing.Any]]:\n",
    "    \"\"\"Extract n-gram features from the corpus of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        n_values (typing.Sequence[int]): the different sizes of n-grams to take (1=unigram, 2=bigram)\n",
    "        lower (bool, optional): whether to lowercase the tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str]]]: the n-gram tokens\n",
    "    \"\"\"\n",
    "    ngrams_list = []\n",
    "\n",
    "    for doc in documents:\n",
    "        ngram_tokens = []\n",
    "\n",
    "        # each review[\"content\"] is a list of sentences,\n",
    "        # where each sentence is a list of (token, pos) pairs\n",
    "        for sentence in doc[\"content\"]:\n",
    "            tokens = [token.lower() if lower else token for token, pos in sentence]\n",
    "\n",
    "            for n in n_values:\n",
    "                for ng in ngrams(tokens, n):\n",
    "                    # join tuple into a single string token\n",
    "                    ngram_tokens.append(\" \".join(ng))\n",
    "\n",
    "        ngrams_list.append(ngram_tokens)\n",
    "\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review bigram tokens: ['two teen', 'teen couples', 'couples go', 'go to', 'to a', 'a church', 'church party', 'party ,', ', drink', 'drink and', 'and then', 'then drive', 'drive .', 'they get', 'get into', 'into an', 'an accident', 'accident .', 'one of', 'of the', 'the guys', 'guys dies', 'dies ,', ', but', 'but his', 'his girlfriend', 'girlfriend continues', 'continues to', 'to see', 'see him', 'him in', 'in her', 'her life', 'life ,', ', and', 'and has', 'has nightmares', 'nightmares .', \"what 's\", \"'s the\", 'the deal', 'deal ?', 'watch the', 'the movie', 'movie and', 'and ``', '`` sorta', \"sorta ''\", \"'' find\", 'find out', 'out ...', '... critique', 'critique :', ': a', 'a mind-fuck', 'mind-fuck movie', 'movie for', 'for the', 'the teen', 'teen generation', 'generation that', 'that touches', 'touches on', 'on a', 'a very', 'very cool', 'cool idea', 'idea ,', ', but', 'but presents', 'presents it', 'it in', 'in a', 'a very', 'very bad', 'bad package', 'package .', 'which is', 'is what', 'what makes', 'makes this', 'this review', 'review an', 'an even', 'even harder', 'harder one', 'one to', 'to write', 'write ,', ', since', 'since i', 'i generally', 'generally applaud', 'applaud films', 'films which', 'which attempt', 'attempt to', 'to break', 'break the', 'the mold', 'mold ,', ', mess', 'mess with', 'with your', 'your head', 'head and', 'and such', 'such -lrb-', '-lrb- lost', 'lost highway', 'highway &', '& memento', 'memento -rrb-', '-rrb- ,', ', but', 'but there', 'there are', 'are good', 'good and', 'and bad', 'bad ways', 'ways of', 'of making', 'making all', 'all types', 'types of', 'of films', 'films ,', ', and', 'and these', 'these folks', 'folks just', 'just did', \"did n't\", \"n't snag\", 'snag this', 'this one', 'one correctly', 'correctly .', 'they seem', 'seem to', 'to have', 'have taken', 'taken this', 'this pretty', 'pretty neat', 'neat concept', 'concept ,', ', but', 'but executed', 'executed it', 'it terribly', 'terribly .', 'so what', 'what are', 'are the', 'the problems', 'problems with', 'with the', 'the movie', 'movie ?', 'well ,', ', its', 'its main', 'main problem', 'problem is', 'is that', 'that it', \"it 's\", \"'s simply\", 'simply too', 'too jumbled', 'jumbled .', 'it starts', 'starts off', 'off ``', '`` normal', \"normal ''\", \"'' but\", 'but then', 'then downshifts', 'downshifts into', 'into this', 'this ``', '`` fantasy', \"fantasy ''\", \"'' world\", 'world in', 'in which', 'which you', 'you ,', ', as', 'as an', 'an audience', 'audience member', 'member ,', ', have', 'have no', 'no idea', 'idea what', \"what 's\", \"'s going\", 'going on', 'on .', 'there are', 'are dreams', 'dreams ,', ', there', 'there are', 'are characters', 'characters coming', 'coming back', 'back from', 'from the', 'the dead', 'dead ,', ', there', 'there are', 'are others', 'others who', 'who look', 'look like', 'like the', 'the dead', 'dead ,', ', there', 'there are', 'are strange', 'strange apparitions', 'apparitions ,', ', there', 'there are', 'are disappearances', 'disappearances ,', ', there', 'there are', 'are a', 'a looooot', 'looooot of', 'of chase', 'chase scenes', 'scenes ,', ', there', 'there are', 'are tons', 'tons of', 'of weird', 'weird things', 'things that', 'that happen', 'happen ,', ', and', 'and most', 'most of', 'of it', 'it is', 'is simply', 'simply not', 'not explained', 'explained .', 'now i', 'i personally', 'personally do', \"do n't\", \"n't mind\", 'mind trying', 'trying to', 'to unravel', 'unravel a', 'a film', 'film every', 'every now', 'now and', 'and then', 'then ,', ', but', 'but when', 'when all', 'all it', 'it does', 'does is', 'is give', 'give me', 'me the', 'the same', 'same clue', 'clue over', 'over and', 'and over', 'over again', 'again ,', ', i', 'i get', 'get kind', 'kind of', 'of fed', 'fed up', 'up after', 'after a', 'a while', 'while ,', ', which', 'which is', 'is this', 'this film', \"film 's\", \"'s biggest\", 'biggest problem', 'problem .', \"it 's\", \"'s obviously\", 'obviously got', 'got this', 'this big', 'big secret', 'secret to', 'to hide', 'hide ,', ', but', 'but it', 'it seems', 'seems to', 'to want', 'want to', 'to hide', 'hide it', 'it completely', 'completely until', 'until its', 'its final', 'final five', 'five minutes', 'minutes .', 'and do', 'do they', 'they make', 'make things', 'things entertaining', 'entertaining ,', ', thrilling', 'thrilling or', 'or even', 'even engaging', 'engaging ,', ', in', 'in the', 'the meantime', 'meantime ?', 'not really', 'really .', 'the sad', 'sad part', 'part is', 'is that', 'that the', 'the arrow', 'arrow and', 'and i', 'i both', 'both dig', 'dig on', 'on flicks', 'flicks like', 'like this', 'this ,', ', so', 'so we', 'we actually', 'actually figured', 'figured most', 'most of', 'of it', 'it out', 'out by', 'by the', 'the half-way', 'half-way point', 'point ,', ', so', 'so all', 'all of', 'of the', 'the strangeness', 'strangeness after', 'after that', 'that did', 'did start', 'start to', 'to make', 'make a', 'a little', 'little bit', 'bit of', 'of sense', 'sense ,', ', but', 'but it', 'it still', 'still did', \"did n't\", \"n't the\", 'the make', 'make the', 'the film', 'film all', 'all that', 'that more', 'more entertaining', 'entertaining .', 'i guess', 'guess the', 'the bottom', 'bottom line', 'line with', 'with movies', 'movies like', 'like this', 'this is', 'is that', 'that you', 'you should', 'should always', 'always make', 'make sure', 'sure that', 'that the', 'the audience', 'audience is', 'is ``', '`` into', 'into it', \"it ''\", \"'' even\", 'even before', 'before they', 'they are', 'are given', 'given the', 'the secret', 'secret password', 'password to', 'to enter', 'enter your', 'your world', 'world of', 'of understanding', 'understanding .', 'i mean', 'mean ,', ', showing', 'showing melissa', 'melissa sagemiller', 'sagemiller running', 'running away', 'away from', 'from visions', 'visions for', 'for about', 'about 20', '20 minutes', 'minutes throughout', 'throughout the', 'the movie', 'movie is', 'is just', 'just plain', 'plain lazy', 'lazy !!', 'okay ,', ', we', 'we get', 'get it', 'it ...', '... there', 'there are', 'are people', 'people chasing', 'chasing her', 'her and', 'and we', 'we do', \"do n't\", \"n't know\", 'know who', 'who they', 'they are', 'are .', 'do we', 'we really', 'really need', 'need to', 'to see', 'see it', 'it over', 'over and', 'and over', 'over again', 'again ?', 'how about', 'about giving', 'giving us', 'us different', 'different scenes', 'scenes offering', 'offering further', 'further insight', 'insight into', 'into all', 'all of', 'of the', 'the strangeness', 'strangeness going', 'going down', 'down in', 'in the', 'the movie', 'movie ?', 'apparently ,', ', the', 'the studio', 'studio took', 'took this', 'this film', 'film away', 'away from', 'from its', 'its director', 'director and', 'and chopped', 'chopped it', 'it up', 'up themselves', 'themselves ,', ', and', 'and it', 'it shows', 'shows .', 'there might', \"might 've\", \"'ve been\", 'been a', 'a pretty', 'pretty decent', 'decent teen', 'teen mind-fuck', 'mind-fuck movie', 'movie in', 'in here', 'here somewhere', 'somewhere ,', ', but', 'but i', 'i guess', 'guess ``', '`` the', 'the suits', \"suits ''\", \"'' decided\", 'decided that', 'that turning', 'turning it', 'it into', 'into a', 'a music', 'music video', 'video with', 'with little', 'little edge', 'edge ,', ', would', 'would make', 'make more', 'more sense', 'sense .', 'the actors', 'actors are', 'are pretty', 'pretty good', 'good for', 'for the', 'the most', 'most part', 'part ,', ', although', 'although wes', 'wes bentley', 'bentley just', 'just seemed', 'seemed to', 'to be', 'be playing', 'playing the', 'the exact', 'exact same', 'same character', 'character that', 'that he', 'he did', 'did in', 'in american', 'american beauty', 'beauty ,', ', only', 'only in', 'in a', 'a new', 'new neighborhood', 'neighborhood .', 'but my', 'my biggest', 'biggest kudos', 'kudos go', 'go out', 'out to', 'to sagemiller', 'sagemiller ,', ', who', 'who holds', 'holds her', 'her own', 'own throughout', 'throughout the', 'the entire', 'entire film', 'film ,', ', and', 'and actually', 'actually has', 'has you', 'you feeling', 'feeling her', 'her character', \"character 's\", \"'s unraveling\", 'unraveling .', 'overall ,', ', the', 'the film', 'film does', \"does n't\", \"n't stick\", 'stick because', 'because it', 'it does', \"does n't\", \"n't entertain\", 'entertain ,', ', it', \"it 's\", \"'s confusing\", 'confusing ,', ', it', 'it rarely', 'rarely excites', 'excites and', 'and it', 'it feels', 'feels pretty', 'pretty redundant', 'redundant for', 'for most', 'most of', 'of its', 'its runtime', 'runtime ,', ', despite', 'despite a', 'a pretty', 'pretty cool', 'cool ending', 'ending and', 'and explanation', 'explanation to', 'to all', 'all of', 'of the', 'the craziness', 'craziness that', 'that came', 'came before', 'before it', 'it .', 'oh ,', ', and', 'and by', 'by the', 'the way', 'way ,', ', this', 'this is', 'is not', 'not a', 'a horror', 'horror or', 'or teen', 'teen slasher', 'slasher flick', 'flick ...', '... it', \"it 's\", \"'s just\", 'just packaged', 'packaged to', 'to look', 'look that', 'that way', 'way because', 'because someone', 'someone is', 'is apparently', 'apparently assuming', 'assuming that', 'that the', 'the genre', 'genre is', 'is still', 'still hot', 'hot with', 'with the', 'the kids', 'kids .', 'it also', 'also wrapped', 'wrapped production', 'production two', 'two years', 'years ago', 'ago and', 'and has', 'has been', 'been sitting', 'sitting on', 'on the', 'the shelves', 'shelves ever', 'ever since', 'since .', 'whatever ...', '... skip', 'skip it', 'it !']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# bigram example\n",
    "X_bi = extract_ngrams(reviews, [2])\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review bigram tokens:\", X_bi[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review trigram tokens: ['two teen couples', 'teen couples go', 'couples go to', 'go to a', 'to a church', 'a church party', 'church party ,', 'party , drink', ', drink and', 'drink and then', 'and then drive', 'then drive .', 'they get into', 'get into an', 'into an accident', 'an accident .', 'one of the', 'of the guys', 'the guys dies', 'guys dies ,', 'dies , but', ', but his', 'but his girlfriend', 'his girlfriend continues', 'girlfriend continues to', 'continues to see', 'to see him', 'see him in', 'him in her', 'in her life', 'her life ,', 'life , and', ', and has', 'and has nightmares', 'has nightmares .', \"what 's the\", \"'s the deal\", 'the deal ?', 'watch the movie', 'the movie and', 'movie and ``', 'and `` sorta', \"`` sorta ''\", \"sorta '' find\", \"'' find out\", 'find out ...', 'out ... critique', '... critique :', 'critique : a', ': a mind-fuck', 'a mind-fuck movie', 'mind-fuck movie for', 'movie for the', 'for the teen', 'the teen generation', 'teen generation that', 'generation that touches', 'that touches on', 'touches on a', 'on a very', 'a very cool', 'very cool idea', 'cool idea ,', 'idea , but', ', but presents', 'but presents it', 'presents it in', 'it in a', 'in a very', 'a very bad', 'very bad package', 'bad package .', 'which is what', 'is what makes', 'what makes this', 'makes this review', 'this review an', 'review an even', 'an even harder', 'even harder one', 'harder one to', 'one to write', 'to write ,', 'write , since', ', since i', 'since i generally', 'i generally applaud', 'generally applaud films', 'applaud films which', 'films which attempt', 'which attempt to', 'attempt to break', 'to break the', 'break the mold', 'the mold ,', 'mold , mess', ', mess with', 'mess with your', 'with your head', 'your head and', 'head and such', 'and such -lrb-', 'such -lrb- lost', '-lrb- lost highway', 'lost highway &', 'highway & memento', '& memento -rrb-', 'memento -rrb- ,', '-rrb- , but', ', but there', 'but there are', 'there are good', 'are good and', 'good and bad', 'and bad ways', 'bad ways of', 'ways of making', 'of making all', 'making all types', 'all types of', 'types of films', 'of films ,', 'films , and', ', and these', 'and these folks', 'these folks just', 'folks just did', \"just did n't\", \"did n't snag\", \"n't snag this\", 'snag this one', 'this one correctly', 'one correctly .', 'they seem to', 'seem to have', 'to have taken', 'have taken this', 'taken this pretty', 'this pretty neat', 'pretty neat concept', 'neat concept ,', 'concept , but', ', but executed', 'but executed it', 'executed it terribly', 'it terribly .', 'so what are', 'what are the', 'are the problems', 'the problems with', 'problems with the', 'with the movie', 'the movie ?', 'well , its', ', its main', 'its main problem', 'main problem is', 'problem is that', 'is that it', \"that it 's\", \"it 's simply\", \"'s simply too\", 'simply too jumbled', 'too jumbled .', 'it starts off', 'starts off ``', 'off `` normal', \"`` normal ''\", \"normal '' but\", \"'' but then\", 'but then downshifts', 'then downshifts into', 'downshifts into this', 'into this ``', 'this `` fantasy', \"`` fantasy ''\", \"fantasy '' world\", \"'' world in\", 'world in which', 'in which you', 'which you ,', 'you , as', ', as an', 'as an audience', 'an audience member', 'audience member ,', 'member , have', ', have no', 'have no idea', 'no idea what', \"idea what 's\", \"what 's going\", \"'s going on\", 'going on .', 'there are dreams', 'are dreams ,', 'dreams , there', ', there are', 'there are characters', 'are characters coming', 'characters coming back', 'coming back from', 'back from the', 'from the dead', 'the dead ,', 'dead , there', ', there are', 'there are others', 'are others who', 'others who look', 'who look like', 'look like the', 'like the dead', 'the dead ,', 'dead , there', ', there are', 'there are strange', 'are strange apparitions', 'strange apparitions ,', 'apparitions , there', ', there are', 'there are disappearances', 'are disappearances ,', 'disappearances , there', ', there are', 'there are a', 'are a looooot', 'a looooot of', 'looooot of chase', 'of chase scenes', 'chase scenes ,', 'scenes , there', ', there are', 'there are tons', 'are tons of', 'tons of weird', 'of weird things', 'weird things that', 'things that happen', 'that happen ,', 'happen , and', ', and most', 'and most of', 'most of it', 'of it is', 'it is simply', 'is simply not', 'simply not explained', 'not explained .', 'now i personally', 'i personally do', \"personally do n't\", \"do n't mind\", \"n't mind trying\", 'mind trying to', 'trying to unravel', 'to unravel a', 'unravel a film', 'a film every', 'film every now', 'every now and', 'now and then', 'and then ,', 'then , but', ', but when', 'but when all', 'when all it', 'all it does', 'it does is', 'does is give', 'is give me', 'give me the', 'me the same', 'the same clue', 'same clue over', 'clue over and', 'over and over', 'and over again', 'over again ,', 'again , i', ', i get', 'i get kind', 'get kind of', 'kind of fed', 'of fed up', 'fed up after', 'up after a', 'after a while', 'a while ,', 'while , which', ', which is', 'which is this', 'is this film', \"this film 's\", \"film 's biggest\", \"'s biggest problem\", 'biggest problem .', \"it 's obviously\", \"'s obviously got\", 'obviously got this', 'got this big', 'this big secret', 'big secret to', 'secret to hide', 'to hide ,', 'hide , but', ', but it', 'but it seems', 'it seems to', 'seems to want', 'to want to', 'want to hide', 'to hide it', 'hide it completely', 'it completely until', 'completely until its', 'until its final', 'its final five', 'final five minutes', 'five minutes .', 'and do they', 'do they make', 'they make things', 'make things entertaining', 'things entertaining ,', 'entertaining , thrilling', ', thrilling or', 'thrilling or even', 'or even engaging', 'even engaging ,', 'engaging , in', ', in the', 'in the meantime', 'the meantime ?', 'not really .', 'the sad part', 'sad part is', 'part is that', 'is that the', 'that the arrow', 'the arrow and', 'arrow and i', 'and i both', 'i both dig', 'both dig on', 'dig on flicks', 'on flicks like', 'flicks like this', 'like this ,', 'this , so', ', so we', 'so we actually', 'we actually figured', 'actually figured most', 'figured most of', 'most of it', 'of it out', 'it out by', 'out by the', 'by the half-way', 'the half-way point', 'half-way point ,', 'point , so', ', so all', 'so all of', 'all of the', 'of the strangeness', 'the strangeness after', 'strangeness after that', 'after that did', 'that did start', 'did start to', 'start to make', 'to make a', 'make a little', 'a little bit', 'little bit of', 'bit of sense', 'of sense ,', 'sense , but', ', but it', 'but it still', 'it still did', \"still did n't\", \"did n't the\", \"n't the make\", 'the make the', 'make the film', 'the film all', 'film all that', 'all that more', 'that more entertaining', 'more entertaining .', 'i guess the', 'guess the bottom', 'the bottom line', 'bottom line with', 'line with movies', 'with movies like', 'movies like this', 'like this is', 'this is that', 'is that you', 'that you should', 'you should always', 'should always make', 'always make sure', 'make sure that', 'sure that the', 'that the audience', 'the audience is', 'audience is ``', 'is `` into', '`` into it', \"into it ''\", \"it '' even\", \"'' even before\", 'even before they', 'before they are', 'they are given', 'are given the', 'given the secret', 'the secret password', 'secret password to', 'password to enter', 'to enter your', 'enter your world', 'your world of', 'world of understanding', 'of understanding .', 'i mean ,', 'mean , showing', ', showing melissa', 'showing melissa sagemiller', 'melissa sagemiller running', 'sagemiller running away', 'running away from', 'away from visions', 'from visions for', 'visions for about', 'for about 20', 'about 20 minutes', '20 minutes throughout', 'minutes throughout the', 'throughout the movie', 'the movie is', 'movie is just', 'is just plain', 'just plain lazy', 'plain lazy !!', 'okay , we', ', we get', 'we get it', 'get it ...', 'it ... there', '... there are', 'there are people', 'are people chasing', 'people chasing her', 'chasing her and', 'her and we', 'and we do', \"we do n't\", \"do n't know\", \"n't know who\", 'know who they', 'who they are', 'they are .', 'do we really', 'we really need', 'really need to', 'need to see', 'to see it', 'see it over', 'it over and', 'over and over', 'and over again', 'over again ?', 'how about giving', 'about giving us', 'giving us different', 'us different scenes', 'different scenes offering', 'scenes offering further', 'offering further insight', 'further insight into', 'insight into all', 'into all of', 'all of the', 'of the strangeness', 'the strangeness going', 'strangeness going down', 'going down in', 'down in the', 'in the movie', 'the movie ?', 'apparently , the', ', the studio', 'the studio took', 'studio took this', 'took this film', 'this film away', 'film away from', 'away from its', 'from its director', 'its director and', 'director and chopped', 'and chopped it', 'chopped it up', 'it up themselves', 'up themselves ,', 'themselves , and', ', and it', 'and it shows', 'it shows .', \"there might 've\", \"might 've been\", \"'ve been a\", 'been a pretty', 'a pretty decent', 'pretty decent teen', 'decent teen mind-fuck', 'teen mind-fuck movie', 'mind-fuck movie in', 'movie in here', 'in here somewhere', 'here somewhere ,', 'somewhere , but', ', but i', 'but i guess', 'i guess ``', 'guess `` the', '`` the suits', \"the suits ''\", \"suits '' decided\", \"'' decided that\", 'decided that turning', 'that turning it', 'turning it into', 'it into a', 'into a music', 'a music video', 'music video with', 'video with little', 'with little edge', 'little edge ,', 'edge , would', ', would make', 'would make more', 'make more sense', 'more sense .', 'the actors are', 'actors are pretty', 'are pretty good', 'pretty good for', 'good for the', 'for the most', 'the most part', 'most part ,', 'part , although', ', although wes', 'although wes bentley', 'wes bentley just', 'bentley just seemed', 'just seemed to', 'seemed to be', 'to be playing', 'be playing the', 'playing the exact', 'the exact same', 'exact same character', 'same character that', 'character that he', 'that he did', 'he did in', 'did in american', 'in american beauty', 'american beauty ,', 'beauty , only', ', only in', 'only in a', 'in a new', 'a new neighborhood', 'new neighborhood .', 'but my biggest', 'my biggest kudos', 'biggest kudos go', 'kudos go out', 'go out to', 'out to sagemiller', 'to sagemiller ,', 'sagemiller , who', ', who holds', 'who holds her', 'holds her own', 'her own throughout', 'own throughout the', 'throughout the entire', 'the entire film', 'entire film ,', 'film , and', ', and actually', 'and actually has', 'actually has you', 'has you feeling', 'you feeling her', 'feeling her character', \"her character 's\", \"character 's unraveling\", \"'s unraveling .\", 'overall , the', ', the film', 'the film does', \"film does n't\", \"does n't stick\", \"n't stick because\", 'stick because it', 'because it does', \"it does n't\", \"does n't entertain\", \"n't entertain ,\", 'entertain , it', \", it 's\", \"it 's confusing\", \"'s confusing ,\", 'confusing , it', ', it rarely', 'it rarely excites', 'rarely excites and', 'excites and it', 'and it feels', 'it feels pretty', 'feels pretty redundant', 'pretty redundant for', 'redundant for most', 'for most of', 'most of its', 'of its runtime', 'its runtime ,', 'runtime , despite', ', despite a', 'despite a pretty', 'a pretty cool', 'pretty cool ending', 'cool ending and', 'ending and explanation', 'and explanation to', 'explanation to all', 'to all of', 'all of the', 'of the craziness', 'the craziness that', 'craziness that came', 'that came before', 'came before it', 'before it .', 'oh , and', ', and by', 'and by the', 'by the way', 'the way ,', 'way , this', ', this is', 'this is not', 'is not a', 'not a horror', 'a horror or', 'horror or teen', 'or teen slasher', 'teen slasher flick', 'slasher flick ...', 'flick ... it', \"... it 's\", \"it 's just\", \"'s just packaged\", 'just packaged to', 'packaged to look', 'to look that', 'look that way', 'that way because', 'way because someone', 'because someone is', 'someone is apparently', 'is apparently assuming', 'apparently assuming that', 'assuming that the', 'that the genre', 'the genre is', 'genre is still', 'is still hot', 'still hot with', 'hot with the', 'with the kids', 'the kids .', 'it also wrapped', 'also wrapped production', 'wrapped production two', 'production two years', 'two years ago', 'years ago and', 'ago and has', 'and has been', 'has been sitting', 'been sitting on', 'sitting on the', 'on the shelves', 'the shelves ever', 'shelves ever since', 'ever since .', 'whatever ... skip', '... skip it', 'skip it !']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# trigram example\n",
    "X_tri = extract_ngrams(reviews, [3])\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review trigram tokens:\", X_tri[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review unigram + bigram + trigram tokens: ['two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'two teen', 'teen couples', 'couples go', 'go to', 'to a', 'a church', 'church party', 'party ,', ', drink', 'drink and', 'and then', 'then drive', 'drive .', 'two teen couples', 'teen couples go', 'couples go to', 'go to a', 'to a church', 'a church party', 'church party ,', 'party , drink', ', drink and', 'drink and then', 'and then drive', 'then drive .', 'they', 'get', 'into', 'an', 'accident', '.', 'they get', 'get into', 'into an', 'an accident', 'accident .', 'they get into', 'get into an', 'into an accident', 'an accident .', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'one of', 'of the', 'the guys', 'guys dies', 'dies ,', ', but', 'but his', 'his girlfriend', 'girlfriend continues', 'continues to', 'to see', 'see him', 'him in', 'in her', 'her life', 'life ,', ', and', 'and has', 'has nightmares', 'nightmares .', 'one of the', 'of the guys', 'the guys dies', 'guys dies ,', 'dies , but', ', but his', 'but his girlfriend', 'his girlfriend continues', 'girlfriend continues to', 'continues to see', 'to see him', 'see him in', 'him in her', 'in her life', 'her life ,', 'life , and', ', and has', 'and has nightmares', 'has nightmares .', 'what', \"'s\", 'the', 'deal', '?', \"what 's\", \"'s the\", 'the deal', 'deal ?', \"what 's the\", \"'s the deal\", 'the deal ?', 'watch', 'the', 'movie', 'and', '``', 'sorta', \"''\", 'find', 'out', '...', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'watch the', 'the movie', 'movie and', 'and ``', '`` sorta', \"sorta ''\", \"'' find\", 'find out', 'out ...', '... critique', 'critique :', ': a', 'a mind-fuck', 'mind-fuck movie', 'movie for', 'for the', 'the teen', 'teen generation', 'generation that', 'that touches', 'touches on', 'on a', 'a very', 'very cool', 'cool idea', 'idea ,', ', but', 'but presents', 'presents it', 'it in', 'in a', 'a very', 'very bad', 'bad package', 'package .', 'watch the movie', 'the movie and', 'movie and ``', 'and `` sorta', \"`` sorta ''\", \"sorta '' find\", \"'' find out\", 'find out ...', 'out ... critique', '... critique :', 'critique : a', ': a mind-fuck', 'a mind-fuck movie', 'mind-fuck movie for', 'movie for the', 'for the teen', 'the teen generation', 'teen generation that', 'generation that touches', 'that touches on', 'touches on a', 'on a very', 'a very cool', 'very cool idea', 'cool idea ,', 'idea , but', ', but presents', 'but presents it', 'presents it in', 'it in a', 'in a very', 'a very bad', 'very bad package', 'bad package .', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '-lrb-', 'lost', 'highway', '&', 'memento', '-rrb-', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'did', \"n't\", 'snag', 'this', 'one', 'correctly', '.', 'which is', 'is what', 'what makes', 'makes this', 'this review', 'review an', 'an even', 'even harder', 'harder one', 'one to', 'to write', 'write ,', ', since', 'since i', 'i generally', 'generally applaud', 'applaud films', 'films which', 'which attempt', 'attempt to', 'to break', 'break the', 'the mold', 'mold ,', ', mess', 'mess with', 'with your', 'your head', 'head and', 'and such', 'such -lrb-', '-lrb- lost', 'lost highway', 'highway &', '& memento', 'memento -rrb-', '-rrb- ,', ', but', 'but there', 'there are', 'are good', 'good and', 'and bad', 'bad ways', 'ways of', 'of making', 'making all', 'all types', 'types of', 'of films', 'films ,', ', and', 'and these', 'these folks', 'folks just', 'just did', \"did n't\", \"n't snag\", 'snag this', 'this one', 'one correctly', 'correctly .', 'which is what', 'is what makes', 'what makes this', 'makes this review', 'this review an', 'review an even', 'an even harder', 'even harder one', 'harder one to', 'one to write', 'to write ,', 'write , since', ', since i', 'since i generally', 'i generally applaud', 'generally applaud films', 'applaud films which', 'films which attempt', 'which attempt to', 'attempt to break', 'to break the', 'break the mold', 'the mold ,', 'mold , mess', ', mess with', 'mess with your', 'with your head', 'your head and', 'head and such', 'and such -lrb-', 'such -lrb- lost', '-lrb- lost highway', 'lost highway &', 'highway & memento', '& memento -rrb-', 'memento -rrb- ,', '-rrb- , but', ', but there', 'but there are', 'there are good', 'are good and', 'good and bad', 'and bad ways', 'bad ways of', 'ways of making', 'of making all', 'making all types', 'all types of', 'types of films', 'of films ,', 'films , and', ', and these', 'and these folks', 'these folks just', 'folks just did', \"just did n't\", \"did n't snag\", \"n't snag this\", 'snag this one', 'this one correctly', 'one correctly .', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'they seem', 'seem to', 'to have', 'have taken', 'taken this', 'this pretty', 'pretty neat', 'neat concept', 'concept ,', ', but', 'but executed', 'executed it', 'it terribly', 'terribly .', 'they seem to', 'seem to have', 'to have taken', 'have taken this', 'taken this pretty', 'this pretty neat', 'pretty neat concept', 'neat concept ,', 'concept , but', ', but executed', 'but executed it', 'executed it terribly', 'it terribly .', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'so what', 'what are', 'are the', 'the problems', 'problems with', 'with the', 'the movie', 'movie ?', 'so what are', 'what are the', 'are the problems', 'the problems with', 'problems with the', 'with the movie', 'the movie ?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'s\", 'simply', 'too', 'jumbled', '.', 'well ,', ', its', 'its main', 'main problem', 'problem is', 'is that', 'that it', \"it 's\", \"'s simply\", 'simply too', 'too jumbled', 'jumbled .', 'well , its', ', its main', 'its main problem', 'main problem is', 'problem is that', 'is that it', \"that it 's\", \"it 's simply\", \"'s simply too\", 'simply too jumbled', 'too jumbled .', 'it', 'starts', 'off', '``', 'normal', \"''\", 'but', 'then', 'downshifts', 'into', 'this', '``', 'fantasy', \"''\", 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'s\", 'going', 'on', '.', 'it starts', 'starts off', 'off ``', '`` normal', \"normal ''\", \"'' but\", 'but then', 'then downshifts', 'downshifts into', 'into this', 'this ``', '`` fantasy', \"fantasy ''\", \"'' world\", 'world in', 'in which', 'which you', 'you ,', ', as', 'as an', 'an audience', 'audience member', 'member ,', ', have', 'have no', 'no idea', 'idea what', \"what 's\", \"'s going\", 'going on', 'on .', 'it starts off', 'starts off ``', 'off `` normal', \"`` normal ''\", \"normal '' but\", \"'' but then\", 'but then downshifts', 'then downshifts into', 'downshifts into this', 'into this ``', 'this `` fantasy', \"`` fantasy ''\", \"fantasy '' world\", \"'' world in\", 'world in which', 'in which you', 'which you ,', 'you , as', ', as an', 'as an audience', 'an audience member', 'audience member ,', 'member , have', ', have no', 'have no idea', 'no idea what', \"idea what 's\", \"what 's going\", \"'s going on\", 'going on .', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'there are', 'are dreams', 'dreams ,', ', there', 'there are', 'are characters', 'characters coming', 'coming back', 'back from', 'from the', 'the dead', 'dead ,', ', there', 'there are', 'are others', 'others who', 'who look', 'look like', 'like the', 'the dead', 'dead ,', ', there', 'there are', 'are strange', 'strange apparitions', 'apparitions ,', ', there', 'there are', 'are disappearances', 'disappearances ,', ', there', 'there are', 'are a', 'a looooot', 'looooot of', 'of chase', 'chase scenes', 'scenes ,', ', there', 'there are', 'are tons', 'tons of', 'of weird', 'weird things', 'things that', 'that happen', 'happen ,', ', and', 'and most', 'most of', 'of it', 'it is', 'is simply', 'simply not', 'not explained', 'explained .', 'there are dreams', 'are dreams ,', 'dreams , there', ', there are', 'there are characters', 'are characters coming', 'characters coming back', 'coming back from', 'back from the', 'from the dead', 'the dead ,', 'dead , there', ', there are', 'there are others', 'are others who', 'others who look', 'who look like', 'look like the', 'like the dead', 'the dead ,', 'dead , there', ', there are', 'there are strange', 'are strange apparitions', 'strange apparitions ,', 'apparitions , there', ', there are', 'there are disappearances', 'are disappearances ,', 'disappearances , there', ', there are', 'there are a', 'are a looooot', 'a looooot of', 'looooot of chase', 'of chase scenes', 'chase scenes ,', 'scenes , there', ', there are', 'there are tons', 'are tons of', 'tons of weird', 'of weird things', 'weird things that', 'things that happen', 'that happen ,', 'happen , and', ', and most', 'and most of', 'most of it', 'of it is', 'it is simply', 'is simply not', 'simply not explained', 'not explained .', 'now', 'i', 'personally', 'do', \"n't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'s\", 'biggest', 'problem', '.', 'now i', 'i personally', 'personally do', \"do n't\", \"n't mind\", 'mind trying', 'trying to', 'to unravel', 'unravel a', 'a film', 'film every', 'every now', 'now and', 'and then', 'then ,', ', but', 'but when', 'when all', 'all it', 'it does', 'does is', 'is give', 'give me', 'me the', 'the same', 'same clue', 'clue over', 'over and', 'and over', 'over again', 'again ,', ', i', 'i get', 'get kind', 'kind of', 'of fed', 'fed up', 'up after', 'after a', 'a while', 'while ,', ', which', 'which is', 'is this', 'this film', \"film 's\", \"'s biggest\", 'biggest problem', 'problem .', 'now i personally', 'i personally do', \"personally do n't\", \"do n't mind\", \"n't mind trying\", 'mind trying to', 'trying to unravel', 'to unravel a', 'unravel a film', 'a film every', 'film every now', 'every now and', 'now and then', 'and then ,', 'then , but', ', but when', 'but when all', 'when all it', 'all it does', 'it does is', 'does is give', 'is give me', 'give me the', 'me the same', 'the same clue', 'same clue over', 'clue over and', 'over and over', 'and over again', 'over again ,', 'again , i', ', i get', 'i get kind', 'get kind of', 'kind of fed', 'of fed up', 'fed up after', 'up after a', 'after a while', 'a while ,', 'while , which', ', which is', 'which is this', 'is this film', \"this film 's\", \"film 's biggest\", \"'s biggest problem\", 'biggest problem .', 'it', \"'s\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', \"it 's\", \"'s obviously\", 'obviously got', 'got this', 'this big', 'big secret', 'secret to', 'to hide', 'hide ,', ', but', 'but it', 'it seems', 'seems to', 'to want', 'want to', 'to hide', 'hide it', 'it completely', 'completely until', 'until its', 'its final', 'final five', 'five minutes', 'minutes .', \"it 's obviously\", \"'s obviously got\", 'obviously got this', 'got this big', 'this big secret', 'big secret to', 'secret to hide', 'to hide ,', 'hide , but', ', but it', 'but it seems', 'it seems to', 'seems to want', 'to want to', 'want to hide', 'to hide it', 'hide it completely', 'it completely until', 'completely until its', 'until its final', 'its final five', 'final five minutes', 'five minutes .', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'and do', 'do they', 'they make', 'make things', 'things entertaining', 'entertaining ,', ', thrilling', 'thrilling or', 'or even', 'even engaging', 'engaging ,', ', in', 'in the', 'the meantime', 'meantime ?', 'and do they', 'do they make', 'they make things', 'make things entertaining', 'things entertaining ,', 'entertaining , thrilling', ', thrilling or', 'thrilling or even', 'or even engaging', 'even engaging ,', 'engaging , in', ', in the', 'in the meantime', 'the meantime ?', 'not', 'really', '.', 'not really', 'really .', 'not really .', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'did', \"n't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'the sad', 'sad part', 'part is', 'is that', 'that the', 'the arrow', 'arrow and', 'and i', 'i both', 'both dig', 'dig on', 'on flicks', 'flicks like', 'like this', 'this ,', ', so', 'so we', 'we actually', 'actually figured', 'figured most', 'most of', 'of it', 'it out', 'out by', 'by the', 'the half-way', 'half-way point', 'point ,', ', so', 'so all', 'all of', 'of the', 'the strangeness', 'strangeness after', 'after that', 'that did', 'did start', 'start to', 'to make', 'make a', 'a little', 'little bit', 'bit of', 'of sense', 'sense ,', ', but', 'but it', 'it still', 'still did', \"did n't\", \"n't the\", 'the make', 'make the', 'the film', 'film all', 'all that', 'that more', 'more entertaining', 'entertaining .', 'the sad part', 'sad part is', 'part is that', 'is that the', 'that the arrow', 'the arrow and', 'arrow and i', 'and i both', 'i both dig', 'both dig on', 'dig on flicks', 'on flicks like', 'flicks like this', 'like this ,', 'this , so', ', so we', 'so we actually', 'we actually figured', 'actually figured most', 'figured most of', 'most of it', 'of it out', 'it out by', 'out by the', 'by the half-way', 'the half-way point', 'half-way point ,', 'point , so', ', so all', 'so all of', 'all of the', 'of the strangeness', 'the strangeness after', 'strangeness after that', 'after that did', 'that did start', 'did start to', 'start to make', 'to make a', 'make a little', 'a little bit', 'little bit of', 'bit of sense', 'of sense ,', 'sense , but', ', but it', 'but it still', 'it still did', \"still did n't\", \"did n't the\", \"n't the make\", 'the make the', 'make the film', 'the film all', 'film all that', 'all that more', 'that more entertaining', 'more entertaining .', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '``', 'into', 'it', \"''\", 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i guess', 'guess the', 'the bottom', 'bottom line', 'line with', 'with movies', 'movies like', 'like this', 'this is', 'is that', 'that you', 'you should', 'should always', 'always make', 'make sure', 'sure that', 'that the', 'the audience', 'audience is', 'is ``', '`` into', 'into it', \"it ''\", \"'' even\", 'even before', 'before they', 'they are', 'are given', 'given the', 'the secret', 'secret password', 'password to', 'to enter', 'enter your', 'your world', 'world of', 'of understanding', 'understanding .', 'i guess the', 'guess the bottom', 'the bottom line', 'bottom line with', 'line with movies', 'with movies like', 'movies like this', 'like this is', 'this is that', 'is that you', 'that you should', 'you should always', 'should always make', 'always make sure', 'make sure that', 'sure that the', 'that the audience', 'the audience is', 'audience is ``', 'is `` into', '`` into it', \"into it ''\", \"it '' even\", \"'' even before\", 'even before they', 'before they are', 'they are given', 'are given the', 'given the secret', 'the secret password', 'secret password to', 'password to enter', 'to enter your', 'enter your world', 'your world of', 'world of understanding', 'of understanding .', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!!', 'i mean', 'mean ,', ', showing', 'showing melissa', 'melissa sagemiller', 'sagemiller running', 'running away', 'away from', 'from visions', 'visions for', 'for about', 'about 20', '20 minutes', 'minutes throughout', 'throughout the', 'the movie', 'movie is', 'is just', 'just plain', 'plain lazy', 'lazy !!', 'i mean ,', 'mean , showing', ', showing melissa', 'showing melissa sagemiller', 'melissa sagemiller running', 'sagemiller running away', 'running away from', 'away from visions', 'from visions for', 'visions for about', 'for about 20', 'about 20 minutes', '20 minutes throughout', 'minutes throughout the', 'throughout the movie', 'the movie is', 'movie is just', 'is just plain', 'just plain lazy', 'plain lazy !!', 'okay', ',', 'we', 'get', 'it', '...', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'do', \"n't\", 'know', 'who', 'they', 'are', '.', 'okay ,', ', we', 'we get', 'get it', 'it ...', '... there', 'there are', 'are people', 'people chasing', 'chasing her', 'her and', 'and we', 'we do', \"do n't\", \"n't know\", 'know who', 'who they', 'they are', 'are .', 'okay , we', ', we get', 'we get it', 'get it ...', 'it ... there', '... there are', 'there are people', 'are people chasing', 'people chasing her', 'chasing her and', 'her and we', 'and we do', \"we do n't\", \"do n't know\", \"n't know who\", 'know who they', 'who they are', 'they are .', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'do we', 'we really', 'really need', 'need to', 'to see', 'see it', 'it over', 'over and', 'and over', 'over again', 'again ?', 'do we really', 'we really need', 'really need to', 'need to see', 'to see it', 'see it over', 'it over and', 'over and over', 'and over again', 'over again ?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'how about', 'about giving', 'giving us', 'us different', 'different scenes', 'scenes offering', 'offering further', 'further insight', 'insight into', 'into all', 'all of', 'of the', 'the strangeness', 'strangeness going', 'going down', 'down in', 'in the', 'the movie', 'movie ?', 'how about giving', 'about giving us', 'giving us different', 'us different scenes', 'different scenes offering', 'scenes offering further', 'offering further insight', 'further insight into', 'insight into all', 'into all of', 'all of the', 'of the strangeness', 'the strangeness going', 'strangeness going down', 'going down in', 'down in the', 'in the movie', 'the movie ?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'apparently ,', ', the', 'the studio', 'studio took', 'took this', 'this film', 'film away', 'away from', 'from its', 'its director', 'director and', 'and chopped', 'chopped it', 'it up', 'up themselves', 'themselves ,', ', and', 'and it', 'it shows', 'shows .', 'apparently , the', ', the studio', 'the studio took', 'studio took this', 'took this film', 'this film away', 'film away from', 'away from its', 'from its director', 'its director and', 'director and chopped', 'and chopped it', 'chopped it up', 'it up themselves', 'up themselves ,', 'themselves , and', ', and it', 'and it shows', 'it shows .', 'there', 'might', \"'ve\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '``', 'the', 'suits', \"''\", 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'there might', \"might 've\", \"'ve been\", 'been a', 'a pretty', 'pretty decent', 'decent teen', 'teen mind-fuck', 'mind-fuck movie', 'movie in', 'in here', 'here somewhere', 'somewhere ,', ', but', 'but i', 'i guess', 'guess ``', '`` the', 'the suits', \"suits ''\", \"'' decided\", 'decided that', 'that turning', 'turning it', 'it into', 'into a', 'a music', 'music video', 'video with', 'with little', 'little edge', 'edge ,', ', would', 'would make', 'make more', 'more sense', 'sense .', \"there might 've\", \"might 've been\", \"'ve been a\", 'been a pretty', 'a pretty decent', 'pretty decent teen', 'decent teen mind-fuck', 'teen mind-fuck movie', 'mind-fuck movie in', 'movie in here', 'in here somewhere', 'here somewhere ,', 'somewhere , but', ', but i', 'but i guess', 'i guess ``', 'guess `` the', '`` the suits', \"the suits ''\", \"suits '' decided\", \"'' decided that\", 'decided that turning', 'that turning it', 'turning it into', 'it into a', 'into a music', 'a music video', 'music video with', 'video with little', 'with little edge', 'little edge ,', 'edge , would', ', would make', 'would make more', 'make more sense', 'more sense .', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'the actors', 'actors are', 'are pretty', 'pretty good', 'good for', 'for the', 'the most', 'most part', 'part ,', ', although', 'although wes', 'wes bentley', 'bentley just', 'just seemed', 'seemed to', 'to be', 'be playing', 'playing the', 'the exact', 'exact same', 'same character', 'character that', 'that he', 'he did', 'did in', 'in american', 'american beauty', 'beauty ,', ', only', 'only in', 'in a', 'a new', 'new neighborhood', 'neighborhood .', 'the actors are', 'actors are pretty', 'are pretty good', 'pretty good for', 'good for the', 'for the most', 'the most part', 'most part ,', 'part , although', ', although wes', 'although wes bentley', 'wes bentley just', 'bentley just seemed', 'just seemed to', 'seemed to be', 'to be playing', 'be playing the', 'playing the exact', 'the exact same', 'exact same character', 'same character that', 'character that he', 'that he did', 'he did in', 'did in american', 'in american beauty', 'american beauty ,', 'beauty , only', ', only in', 'only in a', 'in a new', 'a new neighborhood', 'new neighborhood .', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'s\", 'unraveling', '.', 'but my', 'my biggest', 'biggest kudos', 'kudos go', 'go out', 'out to', 'to sagemiller', 'sagemiller ,', ', who', 'who holds', 'holds her', 'her own', 'own throughout', 'throughout the', 'the entire', 'entire film', 'film ,', ', and', 'and actually', 'actually has', 'has you', 'you feeling', 'feeling her', 'her character', \"character 's\", \"'s unraveling\", 'unraveling .', 'but my biggest', 'my biggest kudos', 'biggest kudos go', 'kudos go out', 'go out to', 'out to sagemiller', 'to sagemiller ,', 'sagemiller , who', ', who holds', 'who holds her', 'holds her own', 'her own throughout', 'own throughout the', 'throughout the entire', 'the entire film', 'entire film ,', 'film , and', ', and actually', 'and actually has', 'actually has you', 'has you feeling', 'you feeling her', 'feeling her character', \"her character 's\", \"character 's unraveling\", \"'s unraveling .\", 'overall', ',', 'the', 'film', 'does', \"n't\", 'stick', 'because', 'it', 'does', \"n't\", 'entertain', ',', 'it', \"'s\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'overall ,', ', the', 'the film', 'film does', \"does n't\", \"n't stick\", 'stick because', 'because it', 'it does', \"does n't\", \"n't entertain\", 'entertain ,', ', it', \"it 's\", \"'s confusing\", 'confusing ,', ', it', 'it rarely', 'rarely excites', 'excites and', 'and it', 'it feels', 'feels pretty', 'pretty redundant', 'redundant for', 'for most', 'most of', 'of its', 'its runtime', 'runtime ,', ', despite', 'despite a', 'a pretty', 'pretty cool', 'cool ending', 'ending and', 'and explanation', 'explanation to', 'to all', 'all of', 'of the', 'the craziness', 'craziness that', 'that came', 'came before', 'before it', 'it .', 'overall , the', ', the film', 'the film does', \"film does n't\", \"does n't stick\", \"n't stick because\", 'stick because it', 'because it does', \"it does n't\", \"does n't entertain\", \"n't entertain ,\", 'entertain , it', \", it 's\", \"it 's confusing\", \"'s confusing ,\", 'confusing , it', ', it rarely', 'it rarely excites', 'rarely excites and', 'excites and it', 'and it feels', 'it feels pretty', 'feels pretty redundant', 'pretty redundant for', 'redundant for most', 'for most of', 'most of its', 'of its runtime', 'its runtime ,', 'runtime , despite', ', despite a', 'despite a pretty', 'a pretty cool', 'pretty cool ending', 'cool ending and', 'ending and explanation', 'and explanation to', 'explanation to all', 'to all of', 'all of the', 'of the craziness', 'the craziness that', 'craziness that came', 'that came before', 'came before it', 'before it .', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '...', 'it', \"'s\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'oh ,', ', and', 'and by', 'by the', 'the way', 'way ,', ', this', 'this is', 'is not', 'not a', 'a horror', 'horror or', 'or teen', 'teen slasher', 'slasher flick', 'flick ...', '... it', \"it 's\", \"'s just\", 'just packaged', 'packaged to', 'to look', 'look that', 'that way', 'way because', 'because someone', 'someone is', 'is apparently', 'apparently assuming', 'assuming that', 'that the', 'the genre', 'genre is', 'is still', 'still hot', 'hot with', 'with the', 'the kids', 'kids .', 'oh , and', ', and by', 'and by the', 'by the way', 'the way ,', 'way , this', ', this is', 'this is not', 'is not a', 'not a horror', 'a horror or', 'horror or teen', 'or teen slasher', 'teen slasher flick', 'slasher flick ...', 'flick ... it', \"... it 's\", \"it 's just\", \"'s just packaged\", 'just packaged to', 'packaged to look', 'to look that', 'look that way', 'that way because', 'way because someone', 'because someone is', 'someone is apparently', 'is apparently assuming', 'apparently assuming that', 'assuming that the', 'that the genre', 'the genre is', 'genre is still', 'is still hot', 'still hot with', 'hot with the', 'with the kids', 'the kids .', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'it also', 'also wrapped', 'wrapped production', 'production two', 'two years', 'years ago', 'ago and', 'and has', 'has been', 'been sitting', 'sitting on', 'on the', 'the shelves', 'shelves ever', 'ever since', 'since .', 'it also wrapped', 'also wrapped production', 'wrapped production two', 'production two years', 'two years ago', 'years ago and', 'ago and has', 'and has been', 'has been sitting', 'been sitting on', 'sitting on the', 'on the shelves', 'the shelves ever', 'shelves ever since', 'ever since .', 'whatever', '...', 'skip', 'it', '!', 'whatever ...', '... skip', 'skip it', 'it !', 'whatever ... skip', '... skip it', 'skip it !']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram + trigram example \n",
    "X_uni_bi_tri = extract_ngrams(reviews, [1, 2, 3])\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review unigram + bigram + trigram tokens:\", X_uni_bi_tri[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3zo7EVzn80K"
   },
   "source": [
    "Now re-train your Naive Bayes classifier with **uni- and bigram** tokens. Report accuracy and compare it with that of the approaches you have previously implemented. Use cross-validation when evaluating your classifier. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "QSlLfRfyn80K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  10%|█         | 1/10 [00:15<02:20, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  20%|██        | 2/10 [00:30<02:00, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  30%|███       | 3/10 [00:38<01:24, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  40%|████      | 4/10 [00:46<01:03, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  50%|█████     | 5/10 [00:54<00:47,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  60%|██████    | 6/10 [01:03<00:37,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  70%|███████   | 7/10 [01:13<00:28,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  80%|████████  | 8/10 [01:24<00:20, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams:  90%|█████████ | 9/10 [01:34<00:10, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni- and Bigrams: 100%|██████████| 10/10 [01:43<00:00, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.7850\n",
      "\n",
      "=== Cross-Validation Results - Naive Bayes Uni- and Bigrams ===\n",
      "Mean accuracy: 0.7755\n",
      "Standard deviation: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uni_bi_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation with Uni- and Bigrams\")):\n",
    "    # extract unigram + bigram train data\n",
    "    X_train = extract_ngrams(train_docs, [1, 2])\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract unigram + bigram test data\n",
    "    X_test = extract_ngrams(test_docs, [1, 2])\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "    # initialize naive bayes classifier with κ=3.5 laplacian smoothing\n",
    "    nb = NaiveBayes(smoothing=3.5)\n",
    "\n",
    "    # fit the naive bayes classifier\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    uni_bi_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== Cross-Validation Results - Naive Bayes Uni- and Bigrams ===\")\n",
    "print(f\"Mean accuracy: {mean(uni_bi_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(uni_bi_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IB8dHbHn80K"
   },
   "source": [
    "Now re-train your Naive Bayes classifier with **uni-, bi- and trigram** tokens. Report accuracy and compare it with that of the approaches you have previously implemented. Use cross-validation when evaluating your classifier. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "jfMvSaxun80K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  10%|█         | 1/10 [00:37<05:33, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  20%|██        | 2/10 [01:28<06:01, 45.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  30%|███       | 3/10 [02:08<05:01, 43.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  40%|████      | 4/10 [02:28<03:24, 34.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  50%|█████     | 5/10 [02:44<02:17, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  60%|██████    | 6/10 [03:01<01:35, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  70%|███████   | 7/10 [03:16<01:03, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  80%|████████  | 8/10 [03:31<00:37, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams:  90%|█████████ | 9/10 [03:55<00:20, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with Uni-, Bi- and Trigrams: 100%|██████████| 10/10 [04:14<00:00, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.7650\n",
      "\n",
      "=== Cross-Validation Results - Naive Bayes Uni-, Bi- and Trigrams ===\n",
      "Mean accuracy: 0.7520\n",
      "Standard deviation: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uni_bi_tri_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation with Uni-, Bi- and Trigrams\")):\n",
    "    # extract unigram + bigram + trigram train data\n",
    "    X_train = extract_ngrams(train_docs, [1, 2, 3])\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract unigram + bigram + trigram test data\n",
    "    X_test = extract_ngrams(test_docs, [1, 2, 3])\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "    # initialize naive bayes classifier with κ=3.5 laplacian smoothing\n",
    "    nb = NaiveBayes(smoothing=3.5)\n",
    "\n",
    "    # fit the naive bayes classifier\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    uni_bi_tri_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== Cross-Validation Results - Naive Bayes Uni-, Bi- and Trigrams ===\")\n",
    "print(f\"Mean accuracy: {mean(uni_bi_tri_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(uni_bi_tri_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "#### (Q2.9): How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How do the number of features increase in the held-out training set (compared to Q2.8)? Do you expect this rate of increase to continue for (much) larger n-grams?\n",
    "\n",
    "Use the held-out training set from training set from Q2.4 for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "> When we add bigrams and trigrams, the number of features increases a lot. In our results, there are about 45,000 unigrams, 465,000 unigrams + bigrams, and more than 1.3 million unigrams + bigrams + trigrams. This shows that the number of features grows much faster than linear, it increases almost exponentially because new word combinations are created for every n-gram. If we keep adding higher-order n-grams, the number of features would keep growing very quickly, making the model larger and more likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OiBHIZvn80L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Vocabulary Size: 45348\n",
      "Unigram + Bigram Vocabulary Size: 465262\n",
      "Unigram + Bigram + Trigram Vocabulary Size: 1346107\n",
      "Increase (Uni → Uni+Bi): 419914\n",
      "Increase (Uni+Bi → Uni+Bi+Tri): 880845\n"
     ]
    }
   ],
   "source": [
    "# use the same training data from q2.4\n",
    "X_train_uni = extract_ngrams(train_reviews, n_values=[1]) # only unigrams, same as X_train_nb and X_train_unigrams\n",
    "X_train_uni_bi = extract_ngrams(train_reviews, n_values=[1, 2]) # unigrams + bigrams\n",
    "X_train_uni_bi_tri = extract_ngrams(train_reviews, n_values=[1, 2, 3]) # unigrams + bigrams + trigrams\n",
    "\n",
    "# compute vocab sizes\n",
    "vocab_uni = set(token for doc in X_train_uni for token in doc)\n",
    "vocab_uni_bi = set(token for doc in X_train_uni_bi for token in doc)\n",
    "vocab_uni_bi_tri = set(token for doc in X_train_uni_bi_tri for token in doc)\n",
    "\n",
    "print(f\"Unigram Vocabulary Size: {len(vocab_uni)}\")\n",
    "print(f\"Unigram + Bigram Vocabulary Size: {len(vocab_uni_bi)}\")\n",
    "print(f\"Unigram + Bigram + Trigram Vocabulary Size: {len(vocab_uni_bi_tri)}\")\n",
    "\n",
    "print(f\"Increase (Uni → Uni+Bi): {len(vocab_uni_bi) - len(vocab_uni)}\")\n",
    "print(f\"Increase (Uni+Bi → Uni+Bi+Tri): {len(vocab_uni_bi_tri) - len(vocab_uni_bi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh"
   },
   "source": [
    "\n",
    "\n",
    "# (3) Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt"
   },
   "source": [
    "Though simple to understand, implement, and debug, one major problem with the Naive Bayes classifier is that its performance deteriorates (becomes skewed) when it is being used with features which are not independent (i.e., are correlated). Another popular classifier that doesn’t assume feature independence is the Support Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr"
   },
   "source": [
    "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (`sklearn.svm.LinearSVC`) using the unigram features collected for Naive Bayes.\n",
    "\n",
    "sklearn's classifiers expect a different data structure than what we've been using so far. Instead of a list of tokens, we'll need to provide a (sparse) [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). Each row represents a document, and each column represents a token. The value in each cell represents how often a token appears in a document.\n",
    "\n",
    "Define a function below that takes a list of tokens and constructs a document-term matrix. While not mandatory, it's recommended to use scipy's `csr_matrix` to produce a sparse matrix representation. This avoids having to keep many 0 values in memory, and can speed up SVM training.\n",
    "\n",
    "Hint: the documentation on the [`csr_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) is very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "xMe4QQWgn80L"
   },
   "outputs": [],
   "source": [
    "def build_term_document_matrix(\n",
    "    train_features: list[list[str]], test_features: list[list[str]]\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Converts a list of token lists to a document-term matrix.\n",
    "\n",
    "    Args:\n",
    "        train_features (list[list[str]]): the training token lists\n",
    "        test_features (list[list[str]]): the testing token lists\n",
    "\n",
    "    Returns:\n",
    "        tuple[array, array]: a tuple of training and testing DTMs\n",
    "    \"\"\"\n",
    "    # build the vocabulary from training data\n",
    "    vocab = {}\n",
    "    for doc in train_features:\n",
    "        for token in doc:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # helper function to convert documents to csr sparse matrix, otherwise we would have a lot of repeated code\n",
    "    def docs_to_csr(docs: list[list[str]]) -> csr_matrix:\n",
    "        rows, cols, data = [], [], []\n",
    "\n",
    "        for row_idx, doc in enumerate(docs):\n",
    "            token_counts = Counter(doc)\n",
    "            for token, count in token_counts.items():\n",
    "                if token in vocab: \n",
    "                    col_idx = vocab[token]\n",
    "                    rows.append(row_idx)\n",
    "                    cols.append(col_idx)\n",
    "                    data.append(count)\n",
    "\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(docs), vocab_size), dtype=np.float64)\n",
    "\n",
    "    X_train = docs_to_csr(train_features)\n",
    "    X_test = docs_to_csr(test_features)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEaKMjsWn80L"
   },
   "source": [
    "Besides a document term matrix, sklearn also expects the labels to be a list of `int`. Using the structure provide, implement a function to convert the `str` labels to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "15Lxwo1xn80M"
   },
   "outputs": [],
   "source": [
    "def convert_labels_to_ints(labels: list[str]) -> list[int]:\n",
    "    \"\"\"Converts a list of \"POS\" or \"NEG\" to 0 or 1.\n",
    "\n",
    "    Args:\n",
    "        labels (list[str]): the list of str labels\n",
    "\n",
    "    Returns:\n",
    "        list[int]: the list of int labels\n",
    "    \"\"\"\n",
    "    label_map = {\"POS\": 1, \"NEG\": 0}\n",
    "    return [label_map[label] for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpn5Zzcen80M"
   },
   "source": [
    "Now train, predict and evaluate an SVM classifier. Compare the classification performance of the SVM classifier to that of the Naive Bayes classifier with smoothing. Use cross-validation to evaluate the performance of the classifiers.\n",
    "\n",
    "You are not required to perform hyperparameter tuning, but it might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "i7-T8pVVn80M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  10%|█         | 1/10 [00:17<02:37, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] SVM Acc=0.8100 | NB Acc=0.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  20%|██        | 2/10 [00:30<01:58, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] SVM Acc=0.7950 | NB Acc=0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  30%|███       | 3/10 [00:47<01:50, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] SVM Acc=0.8000 | NB Acc=0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  40%|████      | 4/10 [00:59<01:25, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] SVM Acc=0.8400 | NB Acc=0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  50%|█████     | 5/10 [01:06<00:58, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] SVM Acc=0.8500 | NB Acc=0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  60%|██████    | 6/10 [01:16<00:44, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] SVM Acc=0.8150 | NB Acc=0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  70%|███████   | 7/10 [01:21<00:27,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] SVM Acc=0.8450 | NB Acc=0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  80%|████████  | 8/10 [01:36<00:21, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] SVM Acc=0.8500 | NB Acc=0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB):  90%|█████████ | 9/10 [01:50<00:11, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] SVM Acc=0.8750 | NB Acc=0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation (SVM vs NB): 100%|██████████| 10/10 [01:55<00:00, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] SVM Acc=0.8400 | NB Acc=0.8150\n",
      "\n",
      "=== Cross-Validation Results (SVM vs NB) ===\n",
      "SVM Mean Accuracy: 0.8320  (std: 0.0257)\n",
      "NB Mean Accuracy: 0.8240  (std: 0.0246)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean, stdev\n",
    "\n",
    "svm_accuracies = []\n",
    "nb_accuracies = []\n",
    "\n",
    "# 10-fold cross-validation\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation (SVM vs NB)\")):\n",
    "    # extract features\n",
    "    X_train_features = extract_unigrams(train_docs)\n",
    "    X_test_features = extract_unigrams(test_docs)\n",
    "\n",
    "    # prepare data for naive bayes\n",
    "    X_train_nb = X_train_features\n",
    "    X_test_nb = X_test_features\n",
    "\n",
    "    # convert to document-term matrices for svm\n",
    "    X_train_svm, X_test_svm = build_term_document_matrix(X_train_features, X_test_features)\n",
    "\n",
    "    # prepare labels for naive bayes\n",
    "    y_train_nb = extract_labels(train_docs)\n",
    "    y_test_nb = extract_labels(test_docs)\n",
    "\n",
    "    # convert labels to ints for svm\n",
    "    y_train_svm = convert_labels_to_ints(y_train)\n",
    "    y_test_svm = convert_labels_to_ints(y_test)\n",
    "\n",
    "    # train and evaluate svm\n",
    "    svm = LinearSVC(random_state=42, max_iter=5000)\n",
    "    svm.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_svm = svm.predict(X_test_svm)\n",
    "    acc_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
    "    svm_accuracies.append(acc_svm)\n",
    "\n",
    "    # train and evaluate naive bayes with κ=3.5 laplace smoothing\n",
    "    nb = NaiveBayes(smoothing=3.5)\n",
    "    nb.fit(X_train_nb, y_train_nb)\n",
    "    y_pred_nb = nb.predict(X_test_nb)\n",
    "    acc_nb = accuracy(y_pred_nb, y_test_nb)\n",
    "    nb_accuracies.append(acc_nb)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] SVM Acc={acc_svm:.4f} | NB Acc={acc_nb:.4f}\")\n",
    "\n",
    "# report overall results\n",
    "print(\"\\n=== Cross-Validation Results (SVM vs NB) ===\")\n",
    "print(f\"SVM Mean Accuracy: {mean(svm_accuracies):.4f}  (std: {stdev(svm_accuracies):.4f})\")\n",
    "print(f\"NB Mean Accuracy: {mean(nb_accuracies):.4f}  (std: {stdev(nb_accuracies):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY"
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "#### (Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAMweWC5n80M"
   },
   "source": [
    "> When using **word + POS features**, the accuracy increased slightly compared to using only unigrams. This shows that part-of-speech information helps because it adds extra context about how each word is used in a sentence. For example, the word **“love”** as a `verb` (“I love this movie”) expresses strong positive sentiment, while as a `noun` in a phrase like “a love of money” can carry a more negative tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTYg1vdJn80M"
   },
   "source": [
    "Once again, we'll need to adjust our `extract_unigrams` function to extract (token, pos) tuples. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "vuNw47Ixn80M"
   },
   "outputs": [],
   "source": [
    "def extract_pos_unigrams(\n",
    "    documents: list[dict], lower: bool = True\n",
    ") -> list[list[tuple[str, str]]]:\n",
    "    \"\"\"Extracts (token, pos) tuples.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        lower (bool, optional): whether to lowercase words. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: the list of (token, pos) tuple lists\n",
    "    \"\"\"\n",
    "    pos_unigrams = []\n",
    "\n",
    "    for doc in documents:\n",
    "        pos_tokens = []\n",
    "\n",
    "        # each review[\"content\"] is a list of sentences,\n",
    "        # where each sentence is a list of (token, pos) pairs\n",
    "        for sentence in doc[\"content\"]:\n",
    "            for token, pos in sentence:\n",
    "                word = token.lower() if lower else token\n",
    "                # combine the word and POS tag into a single feature\n",
    "                combined = f\"{word}_{pos}\"\n",
    "                pos_tokens.append(combined)\n",
    "\n",
    "        pos_unigrams.append(pos_tokens)\n",
    "\n",
    "    return pos_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review POS unigram tokens: ['two_CD', 'teen_JJ', 'couples_NNS', 'go_VBP', 'to_TO', 'a_DT', 'church_NN', 'party_NN', ',_,', 'drink_NN', 'and_CC', 'then_RB', 'drive_NN', '._.', 'they_PRP', 'get_VBP', 'into_IN', 'an_DT', 'accident_NN', '._.', 'one_CD', 'of_IN', 'the_DT', 'guys_NNS', 'dies_VBZ', ',_,', 'but_CC', 'his_PRP$', 'girlfriend_NN', 'continues_VBZ', 'to_TO', 'see_VB', 'him_PRP', 'in_IN', 'her_PRP$', 'life_NN', ',_,', 'and_CC', 'has_VBZ', 'nightmares_NNS', '._.', 'what_WP', \"'s_VBZ\", 'the_DT', 'deal_NN', '?_.', 'watch_VB', 'the_DT', 'movie_NN', 'and_CC', '``_``', 'sorta_NN', \"''_''\", 'find_VB', 'out_RP', '..._:', 'critique_NNP', ':_:', 'a_NNP', 'mind-fuck_JJ', 'movie_NN', 'for_IN', 'the_DT', 'teen_NN', 'generation_NN', 'that_WDT', 'touches_NNS', 'on_IN', 'a_DT', 'very_RB', 'cool_JJ', 'idea_NN', ',_,', 'but_CC', 'presents_VBZ', 'it_PRP', 'in_IN', 'a_DT', 'very_RB', 'bad_JJ', 'package_NN', '._.', 'which_WDT', 'is_VBZ', 'what_WP', 'makes_VBZ', 'this_DT', 'review_NN', 'an_DT', 'even_RB', 'harder_RBR', 'one_CD', 'to_TO', 'write_VB', ',_,', 'since_IN', 'i_PRP', 'generally_RB', 'applaud_VBP', 'films_NNS', 'which_WDT', 'attempt_VBP', 'to_TO', 'break_VB', 'the_DT', 'mold_NN', ',_,', 'mess_NN', 'with_IN', 'your_PRP$', 'head_NN', 'and_CC', 'such_JJ', '-lrb-_-LRB-', 'lost_JJ', 'highway_NNP', '&_CC', 'memento_NNP', '-rrb-_-RRB-', ',_,', 'but_CC', 'there_EX', 'are_VBP', 'good_JJ', 'and_CC', 'bad_JJ', 'ways_NNS', 'of_IN', 'making_VBG', 'all_DT', 'types_NNS', 'of_IN', 'films_NNS', ',_,', 'and_CC', 'these_DT', 'folks_NNS', 'just_RB', 'did_VBD', \"n't_RB\", 'snag_NN', 'this_DT', 'one_CD', 'correctly_RB', '._.', 'they_PRP', 'seem_VBP', 'to_TO', 'have_VB', 'taken_VBN', 'this_DT', 'pretty_RB', 'neat_JJ', 'concept_NN', ',_,', 'but_CC', 'executed_VBD', 'it_PRP', 'terribly_RB', '._.', 'so_RB', 'what_WP', 'are_VBP', 'the_DT', 'problems_NNS', 'with_IN', 'the_DT', 'movie_NN', '?_.', 'well_RB', ',_,', 'its_PRP$', 'main_JJ', 'problem_NN', 'is_VBZ', 'that_IN', 'it_PRP', \"'s_VBZ\", 'simply_RB', 'too_RB', 'jumbled_JJ', '._.', 'it_PRP', 'starts_VBZ', 'off_RP', '``_``', 'normal_JJ', \"''_''\", 'but_CC', 'then_RB', 'downshifts_VBZ', 'into_IN', 'this_DT', '``_``', 'fantasy_NN', \"''_''\", 'world_NN', 'in_IN', 'which_WDT', 'you_PRP', ',_,', 'as_IN', 'an_DT', 'audience_NN', 'member_NN', ',_,', 'have_VBP', 'no_DT', 'idea_NN', 'what_WP', \"'s_VBZ\", 'going_VBG', 'on_RP', '._.', 'there_EX', 'are_VBP', 'dreams_NNS', ',_,', 'there_EX', 'are_VBP', 'characters_NNS', 'coming_VBG', 'back_RB', 'from_IN', 'the_DT', 'dead_JJ', ',_,', 'there_EX', 'are_VBP', 'others_NNS', 'who_WP', 'look_VBP', 'like_IN', 'the_DT', 'dead_JJ', ',_,', 'there_EX', 'are_VBP', 'strange_JJ', 'apparitions_NNS', ',_,', 'there_EX', 'are_VBP', 'disappearances_NNS', ',_,', 'there_EX', 'are_VBP', 'a_DT', 'looooot_NN', 'of_IN', 'chase_NN', 'scenes_NNS', ',_,', 'there_EX', 'are_VBP', 'tons_NNS', 'of_IN', 'weird_JJ', 'things_NNS', 'that_WDT', 'happen_VBP', ',_,', 'and_CC', 'most_JJS', 'of_IN', 'it_PRP', 'is_VBZ', 'simply_RB', 'not_RB', 'explained_VBN', '._.', 'now_RB', 'i_PRP', 'personally_RB', 'do_VBP', \"n't_RB\", 'mind_VB', 'trying_VBG', 'to_TO', 'unravel_VB', 'a_DT', 'film_NN', 'every_DT', 'now_RB', 'and_CC', 'then_RB', ',_,', 'but_CC', 'when_WRB', 'all_DT', 'it_PRP', 'does_VBZ', 'is_VBZ', 'give_VB', 'me_PRP', 'the_DT', 'same_JJ', 'clue_NN', 'over_IN', 'and_CC', 'over_IN', 'again_RB', ',_,', 'i_PRP', 'get_VBP', 'kind_NN', 'of_IN', 'fed_VBN', 'up_RP', 'after_IN', 'a_DT', 'while_NN', ',_,', 'which_WDT', 'is_VBZ', 'this_DT', 'film_NN', \"'s_POS\", 'biggest_JJS', 'problem_NN', '._.', 'it_PRP', \"'s_VBZ\", 'obviously_RB', 'got_VBN', 'this_DT', 'big_JJ', 'secret_NN', 'to_TO', 'hide_VB', ',_,', 'but_CC', 'it_PRP', 'seems_VBZ', 'to_TO', 'want_VB', 'to_TO', 'hide_VB', 'it_PRP', 'completely_RB', 'until_IN', 'its_PRP$', 'final_JJ', 'five_CD', 'minutes_NNS', '._.', 'and_CC', 'do_VBP', 'they_PRP', 'make_VBP', 'things_NNS', 'entertaining_JJ', ',_,', 'thrilling_JJ', 'or_CC', 'even_RB', 'engaging_JJ', ',_,', 'in_IN', 'the_DT', 'meantime_NN', '?_.', 'not_RB', 'really_RB', '._.', 'the_DT', 'sad_JJ', 'part_NN', 'is_VBZ', 'that_IN', 'the_DT', 'arrow_NNP', 'and_CC', 'i_PRP', 'both_DT', 'dig_NN', 'on_IN', 'flicks_NNS', 'like_IN', 'this_DT', ',_,', 'so_IN', 'we_PRP', 'actually_RB', 'figured_VBD', 'most_JJS', 'of_IN', 'it_PRP', 'out_RP', 'by_IN', 'the_DT', 'half-way_NN', 'point_NN', ',_,', 'so_RB', 'all_DT', 'of_IN', 'the_DT', 'strangeness_NN', 'after_IN', 'that_DT', 'did_VBD', 'start_VB', 'to_TO', 'make_VB', 'a_DT', 'little_JJ', 'bit_NN', 'of_IN', 'sense_NN', ',_,', 'but_CC', 'it_PRP', 'still_RB', 'did_VBD', \"n't_RB\", 'the_DT', 'make_VB', 'the_DT', 'film_NN', 'all_DT', 'that_IN', 'more_JJR', 'entertaining_JJ', '._.', 'i_PRP', 'guess_VBP', 'the_DT', 'bottom_JJ', 'line_NN', 'with_IN', 'movies_NNS', 'like_IN', 'this_DT', 'is_VBZ', 'that_IN', 'you_PRP', 'should_MD', 'always_RB', 'make_VB', 'sure_JJ', 'that_IN', 'the_DT', 'audience_NN', 'is_VBZ', '``_``', 'into_IN', 'it_PRP', \"''_''\", 'even_RB', 'before_IN', 'they_PRP', 'are_VBP', 'given_VBN', 'the_DT', 'secret_JJ', 'password_NN', 'to_TO', 'enter_VB', 'your_PRP$', 'world_NN', 'of_IN', 'understanding_NN', '._.', 'i_PRP', 'mean_VBP', ',_,', 'showing_VBG', 'melissa_NNP', 'sagemiller_NNP', 'running_VBG', 'away_RB', 'from_IN', 'visions_NNS', 'for_IN', 'about_RB', '20_CD', 'minutes_NNS', 'throughout_IN', 'the_DT', 'movie_NN', 'is_VBZ', 'just_RB', 'plain_JJ', 'lazy_JJ', '!!_NN', 'okay_UH', ',_,', 'we_PRP', 'get_VBP', 'it_PRP', '..._:', 'there_EX', 'are_VBP', 'people_NNS', 'chasing_VBG', 'her_PRP', 'and_CC', 'we_PRP', 'do_VBP', \"n't_RB\", 'know_VB', 'who_WP', 'they_PRP', 'are_VBP', '._.', 'do_VB', 'we_PRP', 'really_RB', 'need_VBP', 'to_TO', 'see_VB', 'it_PRP', 'over_IN', 'and_CC', 'over_IN', 'again_RB', '?_.', 'how_WRB', 'about_RB', 'giving_VBG', 'us_PRP', 'different_JJ', 'scenes_NNS', 'offering_VBG', 'further_JJ', 'insight_NN', 'into_IN', 'all_DT', 'of_IN', 'the_DT', 'strangeness_NN', 'going_VBG', 'down_RB', 'in_IN', 'the_DT', 'movie_NN', '?_.', 'apparently_RB', ',_,', 'the_DT', 'studio_NN', 'took_VBD', 'this_DT', 'film_NN', 'away_RB', 'from_IN', 'its_PRP$', 'director_NN', 'and_CC', 'chopped_VBD', 'it_PRP', 'up_RP', 'themselves_PRP', ',_,', 'and_CC', 'it_PRP', 'shows_VBZ', '._.', 'there_EX', 'might_MD', \"'ve_VB\", 'been_VBN', 'a_DT', 'pretty_RB', 'decent_JJ', 'teen_NN', 'mind-fuck_NN', 'movie_NN', 'in_IN', 'here_RB', 'somewhere_RB', ',_,', 'but_CC', 'i_PRP', 'guess_VBP', '``_``', 'the_DT', 'suits_NNS', \"''_''\", 'decided_VBD', 'that_IN', 'turning_VBG', 'it_PRP', 'into_IN', 'a_DT', 'music_NN', 'video_NN', 'with_IN', 'little_JJ', 'edge_NN', ',_,', 'would_MD', 'make_VB', 'more_JJR', 'sense_NN', '._.', 'the_DT', 'actors_NNS', 'are_VBP', 'pretty_RB', 'good_JJ', 'for_IN', 'the_DT', 'most_JJS', 'part_NN', ',_,', 'although_IN', 'wes_NNP', 'bentley_NNP', 'just_RB', 'seemed_VBD', 'to_TO', 'be_VB', 'playing_VBG', 'the_DT', 'exact_JJ', 'same_JJ', 'character_NN', 'that_IN', 'he_PRP', 'did_VBD', 'in_IN', 'american_NNP', 'beauty_NNP', ',_,', 'only_RB', 'in_IN', 'a_DT', 'new_JJ', 'neighborhood_NN', '._.', 'but_CC', 'my_PRP$', 'biggest_JJS', 'kudos_NNS', 'go_VBP', 'out_RP', 'to_TO', 'sagemiller_NNP', ',_,', 'who_WP', 'holds_VBZ', 'her_PRP$', 'own_JJ', 'throughout_IN', 'the_DT', 'entire_JJ', 'film_NN', ',_,', 'and_CC', 'actually_RB', 'has_VBZ', 'you_PRP', 'feeling_VBG', 'her_PRP$', 'character_NN', \"'s_POS\", 'unraveling_NN', '._.', 'overall_RB', ',_,', 'the_DT', 'film_NN', 'does_VBZ', \"n't_RB\", 'stick_VB', 'because_IN', 'it_PRP', 'does_VBZ', \"n't_RB\", 'entertain_VB', ',_,', 'it_PRP', \"'s_VBZ\", 'confusing_JJ', ',_,', 'it_PRP', 'rarely_RB', 'excites_VBZ', 'and_CC', 'it_PRP', 'feels_VBZ', 'pretty_RB', 'redundant_JJ', 'for_IN', 'most_JJS', 'of_IN', 'its_PRP$', 'runtime_NN', ',_,', 'despite_IN', 'a_DT', 'pretty_RB', 'cool_JJ', 'ending_VBG', 'and_CC', 'explanation_NN', 'to_TO', 'all_DT', 'of_IN', 'the_DT', 'craziness_NN', 'that_WDT', 'came_VBD', 'before_IN', 'it_PRP', '._.', 'oh_UH', ',_,', 'and_CC', 'by_IN', 'the_DT', 'way_NN', ',_,', 'this_DT', 'is_VBZ', 'not_RB', 'a_DT', 'horror_NN', 'or_CC', 'teen_JJ', 'slasher_NN', 'flick_NN', '..._:', 'it_PRP', \"'s_VBZ\", 'just_RB', 'packaged_VBN', 'to_TO', 'look_VB', 'that_DT', 'way_NN', 'because_IN', 'someone_NN', 'is_VBZ', 'apparently_RB', 'assuming_VBG', 'that_IN', 'the_DT', 'genre_NN', 'is_VBZ', 'still_RB', 'hot_JJ', 'with_IN', 'the_DT', 'kids_NNS', '._.', 'it_PRP', 'also_RB', 'wrapped_VBD', 'production_NN', 'two_CD', 'years_NNS', 'ago_RB', 'and_CC', 'has_VBZ', 'been_VBN', 'sitting_VBG', 'on_IN', 'the_DT', 'shelves_NNS', 'ever_RB', 'since_IN', '._.', 'whatever_WDT', '..._:', 'skip_VB', 'it_PRP', '!_.']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# pos unigrams example\n",
    "X_pos_uni = extract_pos_unigrams(reviews)\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review POS unigram tokens:\", X_pos_uni[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyzxPrRdn80N"
   },
   "source": [
    "Now train, predict and evaluate your SVM classifier, and compare to the SVM with only unigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "30L3aOeon80N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  10%|█         | 1/10 [00:02<00:24,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  20%|██        | 2/10 [00:05<00:23,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  30%|███       | 3/10 [00:14<00:38,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  40%|████      | 4/10 [00:18<00:30,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  50%|█████     | 5/10 [00:28<00:33,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  60%|██████    | 6/10 [00:59<01:00, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  70%|███████   | 7/10 [01:14<00:44, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  80%|████████  | 8/10 [01:30<00:30, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams:  90%|█████████ | 9/10 [01:33<00:11, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM POS Tag Unigrams: 100%|██████████| 10/10 [01:36<00:00,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.8400\n",
      "\n",
      "=== Cross-Validation Results - SVM POS Tag Unigrams ===\n",
      "Mean accuracy: 0.8380\n",
      "Standard deviation: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_uni_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation with SVM POS Tag Unigrams\")):\n",
    "    # extract pos unigram train data\n",
    "    X_train_features = extract_pos_unigrams(train_docs)\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract pos unigram test data\n",
    "    X_test_features = extract_pos_unigrams(test_docs)\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "     # convert to document-term matrices for svm\n",
    "    X_train, X_test = build_term_document_matrix(X_train_features, X_test_features)\n",
    "\n",
    "    # convert labels to ints for svm\n",
    "    y_train = convert_labels_to_ints(y_train)\n",
    "    y_test = convert_labels_to_ints(y_test)\n",
    "\n",
    "    # initialize svm classifier\n",
    "    svm = LinearSVC(random_state=42, max_iter=8000)\n",
    "\n",
    "    # fit the svm classifier\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    pos_uni_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== Cross-Validation Results - SVM POS Tag Unigrams ===\")\n",
    "print(f\"Mean accuracy: {mean(pos_uni_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(pos_uni_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-3w87eMW0w"
   },
   "source": [
    "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOx9ULrEn80N"
   },
   "source": [
    "> When keeping only **open-class words** (nouns, verbs, adjectives, and adverbs), the accuracy increased slightly compared to using all POS-tagged words. This suggests that closed-class words (such as prepositions, determiners, and conjunctions) add little to sentiment analysis and can even introduce noise. Open-class words carry most of the emotional and descriptive meaning in a sentence, while closed-class words mainly serve grammatical functions and rarely reflect sentiment. So by removing them, the classifier focuses on more informative features, leading to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srbgwrXhn80N"
   },
   "source": [
    "For the final time, we'll need to adjust our `extract_pos_unigrams` function to filter (token, pos) tuples for open-class words. Use the provided structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "r5EwGrpon80N"
   },
   "outputs": [],
   "source": [
    "def extract_open_class_pos_unigrams(\n",
    "    documents: list[dict], lower: bool = True\n",
    ") -> list[list[tuple[str, str]]]:\n",
    "    \"\"\"Extracts (token, pos) tuples for open-class words only.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): the list of documents\n",
    "        lower (bool, optional): whether to lowercase words. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[list[tuple[str, str]]]: the list of (token, pos) tuple lists\n",
    "    \"\"\"\n",
    "    # penn treebank open-class pos tags\n",
    "    open_class_tags = {\"NN\", \"NNS\", \"NNP\", \"NNPS\",  # nouns\n",
    "                       \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\",  # verbs\n",
    "                       \"JJ\", \"JJR\", \"JJS\",  # adjectives\n",
    "                       \"RB\", \"RBR\", \"RBS\"}  # adverbs\n",
    "\n",
    "    open_class_pos_unigrams = []\n",
    "\n",
    "    for doc in documents:\n",
    "        pos_tokens = []\n",
    "        \n",
    "        # each review[\"content\"] is a list of sentences,\n",
    "        # where each sentence is a list of (token, pos) pairs\n",
    "        for sentence in doc[\"content\"]:\n",
    "            for token, pos in sentence:\n",
    "                if pos in open_class_tags:\n",
    "                    # combine the word and open class pos tag into a single feature\n",
    "                    word = token.lower() if lower else token\n",
    "                    combined = f\"{word}_{pos}\"\n",
    "                    pos_tokens.append(combined)\n",
    "\n",
    "        open_class_pos_unigrams.append(pos_tokens)\n",
    "\n",
    "    return open_class_pos_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First review open class POS unigram tokens: ['teen_JJ', 'couples_NNS', 'go_VBP', 'church_NN', 'party_NN', 'drink_NN', 'then_RB', 'drive_NN', 'get_VBP', 'accident_NN', 'guys_NNS', 'dies_VBZ', 'girlfriend_NN', 'continues_VBZ', 'see_VB', 'life_NN', 'has_VBZ', 'nightmares_NNS', \"'s_VBZ\", 'deal_NN', 'watch_VB', 'movie_NN', 'sorta_NN', 'find_VB', 'critique_NNP', 'a_NNP', 'mind-fuck_JJ', 'movie_NN', 'teen_NN', 'generation_NN', 'touches_NNS', 'very_RB', 'cool_JJ', 'idea_NN', 'presents_VBZ', 'very_RB', 'bad_JJ', 'package_NN', 'is_VBZ', 'makes_VBZ', 'review_NN', 'even_RB', 'harder_RBR', 'write_VB', 'generally_RB', 'applaud_VBP', 'films_NNS', 'attempt_VBP', 'break_VB', 'mold_NN', 'mess_NN', 'head_NN', 'such_JJ', 'lost_JJ', 'highway_NNP', 'memento_NNP', 'are_VBP', 'good_JJ', 'bad_JJ', 'ways_NNS', 'making_VBG', 'types_NNS', 'films_NNS', 'folks_NNS', 'just_RB', 'did_VBD', \"n't_RB\", 'snag_NN', 'correctly_RB', 'seem_VBP', 'have_VB', 'taken_VBN', 'pretty_RB', 'neat_JJ', 'concept_NN', 'executed_VBD', 'terribly_RB', 'so_RB', 'are_VBP', 'problems_NNS', 'movie_NN', 'well_RB', 'main_JJ', 'problem_NN', 'is_VBZ', \"'s_VBZ\", 'simply_RB', 'too_RB', 'jumbled_JJ', 'starts_VBZ', 'normal_JJ', 'then_RB', 'downshifts_VBZ', 'fantasy_NN', 'world_NN', 'audience_NN', 'member_NN', 'have_VBP', 'idea_NN', \"'s_VBZ\", 'going_VBG', 'are_VBP', 'dreams_NNS', 'are_VBP', 'characters_NNS', 'coming_VBG', 'back_RB', 'dead_JJ', 'are_VBP', 'others_NNS', 'look_VBP', 'dead_JJ', 'are_VBP', 'strange_JJ', 'apparitions_NNS', 'are_VBP', 'disappearances_NNS', 'are_VBP', 'looooot_NN', 'chase_NN', 'scenes_NNS', 'are_VBP', 'tons_NNS', 'weird_JJ', 'things_NNS', 'happen_VBP', 'most_JJS', 'is_VBZ', 'simply_RB', 'not_RB', 'explained_VBN', 'now_RB', 'personally_RB', 'do_VBP', \"n't_RB\", 'mind_VB', 'trying_VBG', 'unravel_VB', 'film_NN', 'now_RB', 'then_RB', 'does_VBZ', 'is_VBZ', 'give_VB', 'same_JJ', 'clue_NN', 'again_RB', 'get_VBP', 'kind_NN', 'fed_VBN', 'while_NN', 'is_VBZ', 'film_NN', 'biggest_JJS', 'problem_NN', \"'s_VBZ\", 'obviously_RB', 'got_VBN', 'big_JJ', 'secret_NN', 'hide_VB', 'seems_VBZ', 'want_VB', 'hide_VB', 'completely_RB', 'final_JJ', 'minutes_NNS', 'do_VBP', 'make_VBP', 'things_NNS', 'entertaining_JJ', 'thrilling_JJ', 'even_RB', 'engaging_JJ', 'meantime_NN', 'not_RB', 'really_RB', 'sad_JJ', 'part_NN', 'is_VBZ', 'arrow_NNP', 'dig_NN', 'flicks_NNS', 'actually_RB', 'figured_VBD', 'most_JJS', 'half-way_NN', 'point_NN', 'so_RB', 'strangeness_NN', 'did_VBD', 'start_VB', 'make_VB', 'little_JJ', 'bit_NN', 'sense_NN', 'still_RB', 'did_VBD', \"n't_RB\", 'make_VB', 'film_NN', 'more_JJR', 'entertaining_JJ', 'guess_VBP', 'bottom_JJ', 'line_NN', 'movies_NNS', 'is_VBZ', 'always_RB', 'make_VB', 'sure_JJ', 'audience_NN', 'is_VBZ', 'even_RB', 'are_VBP', 'given_VBN', 'secret_JJ', 'password_NN', 'enter_VB', 'world_NN', 'understanding_NN', 'mean_VBP', 'showing_VBG', 'melissa_NNP', 'sagemiller_NNP', 'running_VBG', 'away_RB', 'visions_NNS', 'about_RB', 'minutes_NNS', 'movie_NN', 'is_VBZ', 'just_RB', 'plain_JJ', 'lazy_JJ', '!!_NN', 'get_VBP', 'are_VBP', 'people_NNS', 'chasing_VBG', 'do_VBP', \"n't_RB\", 'know_VB', 'are_VBP', 'do_VB', 'really_RB', 'need_VBP', 'see_VB', 'again_RB', 'about_RB', 'giving_VBG', 'different_JJ', 'scenes_NNS', 'offering_VBG', 'further_JJ', 'insight_NN', 'strangeness_NN', 'going_VBG', 'down_RB', 'movie_NN', 'apparently_RB', 'studio_NN', 'took_VBD', 'film_NN', 'away_RB', 'director_NN', 'chopped_VBD', 'shows_VBZ', \"'ve_VB\", 'been_VBN', 'pretty_RB', 'decent_JJ', 'teen_NN', 'mind-fuck_NN', 'movie_NN', 'here_RB', 'somewhere_RB', 'guess_VBP', 'suits_NNS', 'decided_VBD', 'turning_VBG', 'music_NN', 'video_NN', 'little_JJ', 'edge_NN', 'make_VB', 'more_JJR', 'sense_NN', 'actors_NNS', 'are_VBP', 'pretty_RB', 'good_JJ', 'most_JJS', 'part_NN', 'wes_NNP', 'bentley_NNP', 'just_RB', 'seemed_VBD', 'be_VB', 'playing_VBG', 'exact_JJ', 'same_JJ', 'character_NN', 'did_VBD', 'american_NNP', 'beauty_NNP', 'only_RB', 'new_JJ', 'neighborhood_NN', 'biggest_JJS', 'kudos_NNS', 'go_VBP', 'sagemiller_NNP', 'holds_VBZ', 'own_JJ', 'entire_JJ', 'film_NN', 'actually_RB', 'has_VBZ', 'feeling_VBG', 'character_NN', 'unraveling_NN', 'overall_RB', 'film_NN', 'does_VBZ', \"n't_RB\", 'stick_VB', 'does_VBZ', \"n't_RB\", 'entertain_VB', \"'s_VBZ\", 'confusing_JJ', 'rarely_RB', 'excites_VBZ', 'feels_VBZ', 'pretty_RB', 'redundant_JJ', 'most_JJS', 'runtime_NN', 'pretty_RB', 'cool_JJ', 'ending_VBG', 'explanation_NN', 'craziness_NN', 'came_VBD', 'way_NN', 'is_VBZ', 'not_RB', 'horror_NN', 'teen_JJ', 'slasher_NN', 'flick_NN', \"'s_VBZ\", 'just_RB', 'packaged_VBN', 'look_VB', 'way_NN', 'someone_NN', 'is_VBZ', 'apparently_RB', 'assuming_VBG', 'genre_NN', 'is_VBZ', 'still_RB', 'hot_JJ', 'kids_NNS', 'also_RB', 'wrapped_VBD', 'production_NN', 'years_NNS', 'ago_RB', 'has_VBZ', 'been_VBN', 'sitting_VBG', 'shelves_NNS', 'ever_RB', 'skip_VB']\n",
      "First review label: NEG\n"
     ]
    }
   ],
   "source": [
    "# open class pos unigrams example\n",
    "X_open_class_pos_uni = extract_open_class_pos_unigrams(reviews)\n",
    "y = extract_labels(reviews)\n",
    "\n",
    "print(\"First review open class POS unigram tokens:\", X_open_class_pos_uni[0])\n",
    "print(\"First review label:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_myXgvvQn80N"
   },
   "source": [
    "Now train, predict and evaluate your SVM classifier, and compare to the SVM with all POS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "CCUPlPozCYUX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  10%|█         | 1/10 [00:04<00:36,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 01] Accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  20%|██        | 2/10 [00:10<00:42,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 02] Accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  30%|███       | 3/10 [00:12<00:28,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 03] Accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  40%|████      | 4/10 [00:14<00:19,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 04] Accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  50%|█████     | 5/10 [00:17<00:14,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 05] Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  60%|██████    | 6/10 [00:20<00:12,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 06] Accuracy: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  70%|███████   | 7/10 [00:22<00:08,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 07] Accuracy: 0.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  80%|████████  | 8/10 [00:24<00:05,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 08] Accuracy: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams:  90%|█████████ | 9/10 [00:27<00:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 09] Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams: 100%|██████████| 10/10 [00:28<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 10] Accuracy: 0.8350\n",
      "\n",
      "=== Cross-Validation Results - SVM Open Class POS Tag Unigrams ===\n",
      "Mean accuracy: 0.8490\n",
      "Standard deviation: 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "open_class_pos_uni_accuracies = []\n",
    "\n",
    "for i, (train_docs, test_docs) in enumerate(tqdm(folds, desc=\"10-Fold Cross-Validation with SVM Open Class POS Tag Unigrams\")):\n",
    "    # extract open class pos unigram train data\n",
    "    X_train_features = extract_open_class_pos_unigrams(train_docs)\n",
    "    y_train = extract_labels(train_docs)\n",
    "\n",
    "    # extract open class pos unigram test data\n",
    "    X_test_features = extract_open_class_pos_unigrams(test_docs)\n",
    "    y_test = extract_labels(test_docs)\n",
    "\n",
    "     # convert to document-term matrices for svm\n",
    "    X_train, X_test = build_term_document_matrix(X_train_features, X_test_features)\n",
    "\n",
    "    # convert labels to ints for svm\n",
    "    y_train = convert_labels_to_ints(y_train)\n",
    "    y_test = convert_labels_to_ints(y_test)\n",
    "\n",
    "    # initialize svm classifier\n",
    "    svm = LinearSVC(random_state=42, max_iter=8000)\n",
    "\n",
    "    # fit the svm classifier\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # predict sentiments\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    open_class_pos_uni_accuracies.append(acc)\n",
    "\n",
    "    tqdm.write(f\"[Fold {i+1:02d}] Accuracy: {acc:.4f}\")\n",
    "\n",
    "# report overall performance\n",
    "print(\"\\n=== Cross-Validation Results - SVM Open Class POS Tag Unigrams ===\")\n",
    "print(f\"Mean accuracy: {mean(open_class_pos_uni_accuracies):.4f}\")\n",
    "print(f\"Standard deviation: {stdev(open_class_pos_uni_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (4) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our experiments gave us a clear view of what makes features and techniques effective in sentiment analysis. We started with **lexicon-based methods**, which rely on predefined word lists to decide if a text is positive or negative. These methods achieved modest accuracy (around **0.68**) and gave us a simple but useful baseline. Adding weights to strong sentiment words and normalizing by review length to reduce bias slightly improved results to **0.69**, showing that small adjustments can make rule-based models fairer for reviews of different sizes. However, the biggest weakness of such methods is the lack of context understanding. Phrases like “not bad” or “too good to be true” would often be misclassified because the system can only look at individual word polarity. Lexicon-based methods also fail to capture subtle emotional tone or mixed feelings, which are common in real reviews.\n",
    ">\n",
    "> When we moved to **Naive Bayes**, accuracy improved sharply to **0.83–0.85**, showing the power of data-driven learning. The model could estimate how likely each word was to appear in positive or negative reviews, which made it far more adaptable than fixed lexicons. Adding **Laplacian smoothing (κ=3.5)** helped handle unseen words and achieved the best performance. **Stemming** reduced the vocabulary size and made the model slightly more general, though it also lost some nuance by grouping words with different meanings together. When we added **bigrams and trigrams**, the feature space exploded (from about 45k to over 1.3 million features), leading to overfitting and a drop in accuracy to around **0.75**. This reminded us that more features do not always mean better performance. In general, Naive Bayes performed well because of its simplicity and speed, but it struggled with word dependencies and contextual meaning.\n",
    ">\n",
    "> The **SVM classifier** achieved the best overall accuracy at **0.849**, especially when we used **open-class POS features** (nouns, verbs, adjectives, and adverbs). SVMs handle correlated features much better than Naive Bayes and can find more precise decision boundaries between sentiment classes. Adding **POS tags** to the simple SVM gave a small but consistent boost, as it helped the model distinguish between how words were used in context. For instance, “love” as a verb (“I love it”) is clearly positive, while as a noun (“a love of power”) can carry a negative tone. Removing closed-class words like prepositions and conjunctions helped reduce noise and focus on emotionally meaningful terms.\n",
    ">\n",
    "> In summary, our experiments showed that combining simple representations with a bit of linguistic structure gives the best performance. The lexicon-based methods were easy to interpret but limited by their lack of context. Naive Bayes improved accuracy greatly by learning from word distributions, especially with Laplacian smoothing, while the SVM with open-class POS features achieved the best results. Overall, careful feature selection, such as focusing on informative words and using part-of-speech cues, proved more important than model complexity for achieving strong, reliable sentiment classification.\n",
    ">\n",
    "> | Model | Approach | 10-Fold Cross Validation | Mean Test Accuracy |\n",
    "> |:------|:----------|:----------------:|:------------------:|\n",
    "> | **Lexicon-Based** | Threshold=0.8 & Weight=1.0 | No | 0.6790 |\n",
    "> | **Lexicon-Based** | Threshold=0.8 & Best Weight=3.0 | No | 0.6855 |\n",
    "> | **Lexicon-Based** | Normalized Threshold & Best Weight=3.0 | No | 0.6920 |\n",
    "> | **Naive Bayes** | No Laplacian Smoothing | No | 0.8250 |\n",
    "> | **Naive Bayes** | Laplacian Smoothing (κ=1.0) | No | 0.8250 |\n",
    "> | **Naive Bayes** | Laplacian Smoothing (κ=3.5) | No | **0.8450** |\n",
    "> | **Naive Bayes** | Laplacian Smoothing (κ=3.5) & Stemming | Yes | 0.8210 ± 0.0237 |\n",
    "> | **Naive Bayes** | Laplacian Smoothing (κ=3.5) & Uni- + Bigrams | Yes | 0.7755 ± 0.0186 |\n",
    "> | **Naive Bayes** | Laplacian Smoothing (κ=3.5) & Uni- + Bi- + Trigrams | Yes | 0.7520 ± 0.0170 |\n",
    "> | **SVM** | Unigrams | Yes | 0.8320 ± 0.0257 |\n",
    "> | **SVM** | Unigrams & POS Tags | Yes | 0.8380 ± 0.0203 |\n",
    "> | **SVM** | Unigrams & Open-Class POS Tags | Yes | **0.8490 ± 0.0200** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4hweKFFc1gA"
   },
   "source": [
    "# Use of AI tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a21aQFAVc33O"
   },
   "source": [
    "By submitting this notebook for grading you testify that:\n",
    "* AI did not draft an earlier version of your work.\n",
    "* You did not use AI-powered code completion.\n",
    "* You did not implement algorithms suggested by an AI tool.\n",
    "* AI did not revise a version of your work.\n",
    "* You did not implement suggestions made by an AI tool.\n",
    "\n",
    "You in the sentences above refers to you and your team member(s). AI refers to LM-based tools and assistants (e.g. ChatGPT, UvA AI Chat, Gemini, etc.).\n",
    "\n",
    "If you did make use of an AI tool, you should describe the uses you made of it below. Or indicate that no such tool was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-jKEfPuc50G"
   },
   "source": [
    "> No such tool was used."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
